{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Common libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime as dt\n",
    "\n",
    "from src.functions import data_import as dimp\n",
    "from src.functions import data_exploration as dexp\n",
    "from src.functions import data_transformation as dtr\n",
    "from src.functions import metric\n",
    "from src.functions import utils\n",
    "\n",
    "# Graphics\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "import plotly as pty\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import cufflinks as cf\n",
    "cf.set_config_file(offline=True)\n",
    "\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Save images \n",
    "DIR = \"../../TFM/reports/figures/\"\n",
    "WF = \"WF1\"\n",
    "IMAGES_PATH = os.path.join(DIR, WF)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "# Ignore warnings (SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algunas pruebas con diferentes modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para establecer un punto de referencia o *benchmark*, probaremos distintos modelos sin profundizar mucho en el tuning de los hiperparámetros. El objetivo es paulatinamente mejorar el rendimiento de los modelos, a base de modificar las fases del proceso (EDA, Feature Engineering, Hyperparameter Tuning, ...).\n",
    "\n",
    "Elegiremos para conformar nuestro set de entrenamiento:\n",
    "* Un solo NWP.\n",
    "* Run de las 00h.\n",
    "* Día D-1.\n",
    "* Variables meteorológicas de partida `time`, `U`, `V` y `T`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "from kedro.io import DataCatalog\n",
    "from kedro.extras.datasets.pandas import CSVDataSet\n",
    "\n",
    "X_train = context.catalog.load('X_train_raw')\n",
    "X_test = context.catalog.load('X_test_raw')\n",
    "Y_train = context.catalog.load('y_train_raw')\n",
    "\n",
    "X_train['Time'] = pd.to_datetime(X_train['Time'], format='%d/%m/%Y %H:%M')\n",
    "X_test['Time'] = pd.to_datetime(X_test['Time'], format='%d/%m/%Y %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def input_missing_values(df, cols):\n",
    "    \n",
    "    regex = 'NWP(?P<NWP>\\d{1})_(?P<run>\\d{2}h)_(?P<fc_day>D\\W?\\d?)_(?P<weather_var>\\w{1,4})'\n",
    "    p = re.compile(regex)  \n",
    "    \n",
    "    NWP_met_vars_dict = {\n",
    "        '1': ['U','V','T'],\n",
    "        '2': ['U','V'],\n",
    "        '3': ['U','V','T'],\n",
    "        '4': ['U','V','CLCT']\n",
    "    }\n",
    "    \n",
    "    for col in reversed(cols):\n",
    "        m = p.match(col)\n",
    "        col_name = 'NWP' + m.group('NWP') + '_' +  m.group('run') + '_' + m.group('fc_day') + '_' + m.group('weather_var')\n",
    "\n",
    "        for key, value in NWP_met_vars_dict.items():\n",
    "            for i in value:\n",
    "                if m.group('NWP') == key and m.group('weather_var') == i:\n",
    "                    df['NWP'+ key + '_' + i] = df['NWP'+ key + '_' + i].fillna(df[col_name])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from metpy import calc\n",
    "from metpy.units import units\n",
    "\n",
    "# function to obtain the module of wind velocity\n",
    "get_wind_velmod = lambda x : float(calc.wind_speed(\n",
    "    x.U * units.meter/units.second, \n",
    "    x.V * units.meter/units.second\n",
    ").magnitude)\n",
    "\n",
    "# function to obtain the wind direction\n",
    "get_wind_dir = lambda x : float(calc.wind_direction(\n",
    "    x.U * units.meter/units.second, \n",
    "    x.V * units.meter/units.second, \n",
    "    convention=\"from\"\n",
    ").magnitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operational_analysis.toolkits import filters\n",
    "from operational_analysis.toolkits import power_curve\n",
    "\n",
    "def plot_flagged_pc(ws, p, flag_bool, alpha):\n",
    "    plt.scatter(ws, p, s = 3, alpha = alpha)\n",
    "    plt.scatter(ws[flag_bool], p[flag_bool], s = 3, c = 'red')\n",
    "    plt.xlabel('Wind speed (m/s)')\n",
    "    plt.ylabel('Power (kW)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_by_date(date, X, y):\n",
    "    \"\"\"\n",
    "    It splits X and y sets by a 'Time' value \n",
    "    into sets for training and testing. \n",
    "        - Return: a dictionary with the four sets\n",
    "                  (X_train, y_train, X_test, y_test)\n",
    "    \"\"\"\n",
    "    sets = {}\n",
    "    date_cut = dt.datetime.strptime(date, '%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    X_test = X[X['Time'] > date_cut]\n",
    "    X_train = X[X['Time'] <= date_cut]\n",
    "    y_train = y.loc[X_train.ID - 1]\n",
    "    y_test = y.loc[X_test.ID - 1]\n",
    "    \n",
    "    sets['X_train'] = X_train\n",
    "    sets['X_test'] = X_test\n",
    "    sets['y_train'] = y_train\n",
    "    sets['y_test'] = y_test\n",
    "    \n",
    "    return sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión con KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Automatización de pre-procesado, modelización, validación y prediccion\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor \n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.functions import metric\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import r2_score, median_absolute_error\n",
    "import hdbscan\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.cluster import DBSCAN\n",
    "import windpowerlib as wp\n",
    "from windpowerlib import power_curves as pc\n",
    "import scipy as sp\n",
    "from operational_analysis.toolkits import filters\n",
    "from operational_analysis.toolkits import power_curve\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.WARN)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def eval_metrics(actual, pred):\n",
    "    rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "    mae = mean_absolute_error(actual, pred)\n",
    "    r2 = r2_score(actual, pred)\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "    return rmse, mae, r2, cape\n",
    "\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = ['WF1']\n",
    "\n",
    "# lista para guardar los modelos y los scores de cada WF\n",
    "models = []\n",
    "scores_list = []\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "# fichero de salida\n",
    "output = open('results.txt', 'a+')\n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "\n",
    "    # We'll add new columns NWPX_<met_var> without missing values\n",
    "    # new cols: NWP1_U, NWP1_V, NWP1_T, NWP2_U, NWP2_V, NWP3_U, NWP3_V, NWP3_T, NWP4_U, NWP4_V, NWP4_CLCT\n",
    "    \n",
    "    new_cols = ['NWP1_U','NWP1_V','NWP1_T','NWP2_U',\n",
    "                'NWP2_V','NWP3_U','NWP3_V','NWP3_T',\n",
    "                'NWP4_U','NWP4_V','NWP4_CLCT']\n",
    "\n",
    "    def add_new_cols(new_cols, df):\n",
    "        for col in new_cols:\n",
    "            df[col] = np.nan    \n",
    "            \n",
    "    add_new_cols(new_cols, X_train_cpy)\n",
    "    add_new_cols(new_cols, X_test_cpy)\n",
    "    \n",
    "    cols_train = X_train.columns[3:]\n",
    "    cols_test = X_test.columns[3:-9]\n",
    "    X_train_cpy = input_missing_values(X_train_cpy, cols_train)\n",
    "    X_test_cpy = input_missing_values(X_test_cpy, cols_test)\n",
    "    \n",
    "    col_list = ['NWP2_U','NWP2_V','NWP3_U','NWP3_V','NWP3_T']\n",
    "    X_train_cpy.index = X_train_cpy['Time']\n",
    "    \n",
    "    del X_train_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_train_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=2,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "        \n",
    "    X_train_cpy.reset_index(inplace=True)\n",
    "    \n",
    "    X_test_cpy.index = X_test_cpy['Time']\n",
    "    \n",
    "    del X_test_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_test_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=2,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "    \n",
    "        \n",
    "    X_test_cpy.reset_index(inplace=True) \n",
    "    \n",
    "\n",
    "    X_train_cpy['U'] = X_train_cpy.NWP1_U\n",
    "    X_train_cpy['V'] = X_train_cpy.NWP1_V\n",
    "    X_train_cpy['T'] = X_train_cpy.NWP3_T\n",
    "    X_train_cpy['CLCT'] = X_train_cpy.NWP4_CLCT\n",
    "    \n",
    "    X_test_cpy['U'] = X_test_cpy.NWP1_U\n",
    "    X_test_cpy['V'] = X_test_cpy.NWP1_V\n",
    "    X_test_cpy['T'] = X_test_cpy.NWP3_T\n",
    "    X_test_cpy['CLCT'] = X_test_cpy.NWP4_CLCT\n",
    " \n",
    "    X_train_cpy = X_train_cpy[['ID','Time','U','V','T','CLCT']]\n",
    "    X_test_cpy = X_test_cpy[['ID','Time','U','V','T','CLCT']]\n",
    "\n",
    "    ####### Limpieza de outliers ########\n",
    "    X_train_cpy.loc[X_train_cpy['CLCT'] < 0, 'CLCT'] = 0.0\n",
    "    X_test_cpy.loc[X_test_cpy['CLCT'] < 0, 'CLCT'] = 0.0\n",
    "    \n",
    "    # añadir columna Production\n",
    "    X_train_cpy['Production'] = list(Y_train_cpy['Production'])\n",
    "    \n",
    "    # calcular módulo velocidad del viento\n",
    "    X_train_cpy['vel'] = X_train_cpy.apply(get_wind_velmod, axis=1)\n",
    "    \n",
    "    X_ = X_train_cpy[['vel','Production']]\n",
    "    \n",
    "    # parámetros por granja\n",
    "    if WF == 'WF1':\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.25\n",
    "        frac_std = 0.85\n",
    "        threshold_type = 'scalar'\n",
    "        bottom_max = 6.\n",
    "    elif WF == \"WF2\":\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.15\n",
    "        frac_std = 0.8\n",
    "        threshold_type = 'scalar'\n",
    "        bottom_max = 5.5\n",
    "    elif WF == \"WF3\":\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.1\n",
    "        frac_std = 0.8\n",
    "        threshold_type = 'scalar'\n",
    "        bottom_max = 5.5\n",
    "    elif WF == \"WF4\":\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.1\n",
    "        frac_std = 0.95\n",
    "        threshold_type = 'scalar'\n",
    "        bottom_max = 5.3\n",
    "    elif WF == \"WF5\":\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.1\n",
    "        frac_std = 0.55\n",
    "        threshold_type = 'scalar'\n",
    "        bottom_max = 5.\n",
    "    else:\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.05\n",
    "        frac_std = 1.\n",
    "        threshold_type = 'std'\n",
    "        bottom_max = 5.\n",
    "\n",
    "        \n",
    "    # top-curve stacked outliers\n",
    "    top_stacked = filters.window_range_flag(\n",
    "        X_.Production, top_frac_max * X_.Production.max(), X_.Production.max(), X_.vel, 12.5, 2000.\n",
    "    )\n",
    "    \n",
    "    # sparse outliers\n",
    "    max_bin = 0.97*X_['Production'].max()\n",
    "    sparse_outliers = filters.bin_filter(\n",
    "        X_.Production, X_.vel, sparse_bin_width, frac_std * X_.Production.std(), 'median', 0.1, max_bin, threshold_type, 'all')\n",
    "    \n",
    "    # bottom-curve stacked outliers\n",
    "    bottom_stacked = filters.window_range_flag(X_.vel, bottom_max, 40, X_.Production, 0.05, 2000.)\n",
    "    \n",
    "    # deleting outliers\n",
    "    X_.vel = X_.vel[(~top_stacked) & (~sparse_outliers) & (~bottom_stacked)]\n",
    "    X_.Production = X_.Production[(~top_stacked) & (~sparse_outliers) & (~bottom_stacked)]\n",
    "    plot_flagged_pc(X_.vel, X_.Production, np.repeat('True', len(X_.vel)), 0.7)\n",
    "    \n",
    "    # seleccionar filas correspondientes a los no-outliers\n",
    "    X_train_cpy = X_train_cpy.loc[X_train_cpy['vel'].isin(X_.vel)]\n",
    "    Y_train_cpy = Y_train_cpy.loc[Y_train_cpy['ID'].isin(X_train_cpy['ID'])]\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "\n",
    "    \n",
    "    # Eliminamos las columnas 'vel' y 'Production'\n",
    "    del X_train_cpy['vel']\n",
    "    del X_train_cpy['Production']\n",
    "    \n",
    "    ###################################\n",
    "\n",
    "    # Pre-procesado de los datos\n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [(\n",
    "                                        'drop_columns', 'drop',\n",
    "                                        ['ID','Time','U','V','T','CLCT',\n",
    "                                         'w_dir_sin','w_dir',\n",
    "                                         'hour_cos','hour_sin','hour',\n",
    "                                         'month','month_sin','month_cos'\n",
    "                                        ])\n",
    "                                    ])\n",
    "\n",
    "    # definir pipeline\n",
    "    tt = Pipeline(steps=[\n",
    "        #('powertransformer', PowerTransformer(method='yeo-johnson', standardize=True)),\n",
    "        ('normalization', MinMaxScaler(feature_range=(0, 1))) \n",
    "    ])\n",
    "\n",
    "    prepare_data_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', dtr.NewFeaturesAdder(add_time_feat=True, add_cycl_feat=True, add_inv_T=False)), \n",
    "        ('pre_processing', pre_process),\n",
    "        ('powertransformer', PowerTransformer(method='yeo-johnson', standardize=True)),\n",
    "        # ('normalization', MinMaxScaler(feature_range=(0, 1)))    \n",
    "    ])\n",
    "    \n",
    "    # apply pipeline\n",
    "    X_train_pped = prepare_data_pipeline.fit_transform(X_train_cpy)\n",
    "    X_test_pped = prepare_data_pipeline.fit_transform(X_test_cpy)\n",
    "    \n",
    "    # create custome scorer: CAPE\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "    \n",
    "    # Modeling KNN: GridSearch implementation\n",
    "    param_grid = {\n",
    "        'n_neighbors': list(range(1,50,2)),\n",
    "        'algorithm':['auto', 'kd_tree'],\n",
    "        'weights': ['uniform','distance'],\n",
    "        'metric': ['cityblock','mahalanobis','euclidean'],\n",
    "        'p': [1,2]\n",
    "    }\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=7)\n",
    "    knn_reg = KNeighborsRegressor()\n",
    "    grid_search_knn = GridSearchCV(\n",
    "        knn_reg, \n",
    "        param_grid, \n",
    "        cv= 7,\n",
    "        n_jobs=-1,\n",
    "        scoring=cape_scorer\n",
    "    )\n",
    "\n",
    "    grid_search_knn.fit(X_train_pped, Y_train_cpy)\n",
    "\n",
    "    # Reentrenamos sin validación cruzada utilizando los mejores \n",
    "    # parámetros obtenidos con la validación cruzada\n",
    "\n",
    "    knn_reg2 = KNeighborsRegressor(algorithm=grid_search_knn.best_params_['algorithm'],\n",
    "                                   n_neighbors=grid_search_knn.best_params_['n_neighbors'],\n",
    "                                   weights=grid_search_knn.best_params_['weights'],\n",
    "                                   metric=grid_search_knn.best_params_['metric'],\n",
    "                                   p=grid_search_knn.best_params_['p'])\n",
    "\n",
    "    ttreg = TransformedTargetRegressor(regressor=knn_reg2, \n",
    "                                       transformer=tt, \n",
    "                                       check_inverse=False)\n",
    "\n",
    "    ttreg.fit(X_train_pped, Y_train_cpy)\n",
    "    # knn_reg2.fit(X_train_pped, Y_train_cpy)\n",
    "    \n",
    "    # evaluación sobre el conjunto de test\n",
    "    predictions = ttreg.predict(X_test_pped)\n",
    "\n",
    "    # build prediction matrix (ID,Production)\n",
    "    pred_matrix = np.stack((np.array(ID_test).astype(int), predictions), axis=-1)\n",
    "    df_pred = pd.DataFrame(data=pred_matrix.reshape(-1,2), columns=['ID','Production'])\n",
    "\n",
    "    # añadimos las predicciones al dataframe final que contendrá las de todas las WF\n",
    "    submission_df = submission_df.append(df_pred, ignore_index=True) \n",
    "    print('CAPE:', -grid_search_knn.best_score_)\n",
    "    print('Predictions for {} has been added to submission_df'.format(WF))\n",
    "    print('---------------')\n",
    "    \n",
    "# generamos fichero csv para el submission\n",
    "submission_df.to_csv(\"../../TFM/models/knn_vD.csv\", index=False, sep=\",\") \n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión con ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden=1, n_neurons = 30, learning_rate = 3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    \n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "        \n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(lr = learning_rate)\n",
    "    model.compile(loss = \"mse\", optimizer = optimizer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Automatización de pre-procesado, modelización, validación y prediccion\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor \n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.functions import metric\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import r2_score, median_absolute_error\n",
    "import hdbscan\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.cluster import DBSCAN\n",
    "import windpowerlib as wp\n",
    "from windpowerlib import power_curves as pc\n",
    "import scipy as sp\n",
    "from operational_analysis.toolkits import filters\n",
    "from operational_analysis.toolkits import power_curve\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.WARN)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def eval_metrics(actual, pred):\n",
    "    rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "    mae = mean_absolute_error(actual, pred)\n",
    "    r2 = r2_score(actual, pred)\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "    return rmse, mae, r2, cape\n",
    "\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = [1]\n",
    "\n",
    "# lista para guardar los modelos y los scores de cada WF\n",
    "models = []\n",
    "scores_list = []\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "\n",
    "    # We'll add new columns NWPX_<met_var> without missing values\n",
    "    # new cols: NWP1_U, NWP1_V, NWP1_T, NWP2_U, NWP2_V, NWP3_U, NWP3_V, NWP3_T, NWP4_U, NWP4_V, NWP4_CLCT\n",
    "    \n",
    "    new_cols = ['NWP1_U','NWP1_V','NWP1_T','NWP2_U',\n",
    "                'NWP2_V','NWP3_U','NWP3_V','NWP3_T',\n",
    "                'NWP4_U','NWP4_V','NWP4_CLCT']\n",
    "\n",
    "    def add_new_cols(new_cols, df):\n",
    "        for col in new_cols:\n",
    "            df[col] = np.nan    \n",
    "            \n",
    "    add_new_cols(new_cols, X_train_cpy)\n",
    "    add_new_cols(new_cols, X_test_cpy)\n",
    "    \n",
    "    cols_train = X_train.columns[3:]\n",
    "    cols_test = X_test.columns[3:-9]\n",
    "    X_train_cpy = input_missing_values(X_train_cpy, cols_train)\n",
    "    X_test_cpy = input_missing_values(X_test_cpy, cols_test)\n",
    "    \n",
    "    col_list = ['NWP2_U','NWP2_V','NWP3_U','NWP3_V','NWP3_T']\n",
    "    X_train_cpy.index = X_train_cpy['Time']\n",
    "    \n",
    "    del X_train_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_train_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=2,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "        \n",
    "    X_train_cpy.reset_index(inplace=True)\n",
    "    \n",
    "    X_test_cpy.index = X_test_cpy['Time']\n",
    "    \n",
    "    del X_test_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_test_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=100,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "    \n",
    "        \n",
    "    X_test_cpy.reset_index(inplace=True) \n",
    "    \n",
    "\n",
    "    X_train_cpy['U'] = X_train_cpy.NWP1_U\n",
    "    X_train_cpy['V'] = X_train_cpy.NWP1_V\n",
    "    X_train_cpy['T'] = X_train_cpy.NWP3_T\n",
    "    X_train_cpy['CLCT'] = X_train_cpy.NWP4_CLCT\n",
    "    \n",
    "    X_test_cpy['U'] = X_test_cpy.NWP1_U\n",
    "    X_test_cpy['V'] = X_test_cpy.NWP1_V\n",
    "    X_test_cpy['T'] = X_test_cpy.NWP3_T\n",
    "    X_test_cpy['CLCT'] = X_test_cpy.NWP4_CLCT\n",
    " \n",
    "    X_train_cpy = X_train_cpy[['ID','Time','U','V','T','CLCT']]\n",
    "    X_test_cpy = X_test_cpy[['ID','Time','U','V','T','CLCT']]\n",
    "\n",
    "    ####### Limpieza de outliers ########\n",
    "    X_train_cpy.loc[X_train_cpy['CLCT'] < 0, 'CLCT'] = 0.0\n",
    "    X_test_cpy.loc[X_test_cpy['CLCT'] < 0, 'CLCT'] = 0.0\n",
    "    \n",
    "    # añadir columna Production\n",
    "    X_train_cpy['Production'] = list(Y_train_cpy['Production'])\n",
    "    \n",
    "    # calcular módulo velocidad del viento\n",
    "    X_train_cpy['vel'] = X_train_cpy.apply(get_wind_velmod, axis=1)\n",
    "    \n",
    "    X_ = X_train_cpy[['vel','Production']]\n",
    "    \n",
    "    # parámetros por granja\n",
    "    if WF == 'WF1':\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.25\n",
    "        frac_std = 0.85\n",
    "        threshold_type = 'scalar'\n",
    "        bottom_max = 6.\n",
    "    elif WF == \"WF2\":\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.15\n",
    "        frac_std = 0.8\n",
    "        threshold_type = 'scalar'\n",
    "        bottom_max = 5.5\n",
    "    elif WF == \"WF3\":\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.1\n",
    "        frac_std = 0.8\n",
    "        threshold_type = 'scalar'\n",
    "        bottom_max = 5.5\n",
    "    elif WF == \"WF4\":\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.1\n",
    "        frac_std = 0.95\n",
    "        threshold_type = 'scalar'\n",
    "        bottom_max = 5.3\n",
    "    elif WF == \"WF5\":\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.1\n",
    "        frac_std = 0.55\n",
    "        threshold_type = 'scalar'\n",
    "        bottom_max = 5.\n",
    "    else:\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.05\n",
    "        frac_std = 1.\n",
    "        threshold_type = 'std'\n",
    "        bottom_max = 5.\n",
    "\n",
    "        \n",
    "    # top-curve stacked outliers\n",
    "    top_stacked = filters.window_range_flag(\n",
    "        X_.Production, top_frac_max * X_.Production.max(), X_.Production.max(), X_.vel, 12.5, 2000.\n",
    "    )\n",
    "    \n",
    "    # sparse outliers\n",
    "    max_bin = 0.97*X_['Production'].max()\n",
    "    sparse_outliers = filters.bin_filter(\n",
    "        X_.Production, X_.vel, sparse_bin_width, frac_std * X_.Production.std(), 'median', 0.1, max_bin, threshold_type, 'all')\n",
    "    \n",
    "    # bottom-curve stacked outliers\n",
    "    bottom_stacked = filters.window_range_flag(X_.vel, bottom_max, 40, X_.Production, 0.05, 2000.)\n",
    "    \n",
    "    # deleting outliers\n",
    "    X_.vel = X_.vel[(~top_stacked) & (~sparse_outliers) & (~bottom_stacked)]\n",
    "    X_.Production = X_.Production[(~top_stacked) & (~sparse_outliers) & (~bottom_stacked)]\n",
    "    plot_flagged_pc(X_.vel, X_.Production, np.repeat('True', len(X_.vel)), 0.7)\n",
    "    \n",
    "    # seleccionar filas correspondientes a los no-outliers\n",
    "    X_train_cpy = X_train_cpy.loc[X_train_cpy['vel'].isin(X_.vel)]\n",
    "    Y_train_cpy = Y_train_cpy.loc[Y_train_cpy['ID'].isin(X_train_cpy['ID'])]\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "\n",
    "    \n",
    "    # Eliminamos las columnas 'vel' y 'Production'\n",
    "    del X_train_cpy['vel']\n",
    "    del X_train_cpy['Production']\n",
    "    \n",
    "    ###################################\n",
    "    \n",
    "    train_test_dfs = split_data_by_date('2018-12-15 23:00:00', X_train_cpy, Y_train_cpy)\n",
    "    X_train_2 = train_test_dfs.get('X_train')\n",
    "    X_val = train_test_dfs.get('X_test')\n",
    "    y_train_2 = train_test_dfs.get('y_train')\n",
    "    y_val = train_test_dfs.get('y_test')\n",
    "    \n",
    "    drop_lst = ['ID','Time','U','V','CLCT']\n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [(\n",
    "                                        'drop_columns', 'drop', drop_lst)\n",
    "                                    ])\n",
    "    prepare_data_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', dtr.NewFeaturesAdder(add_time_feat=False, add_cycl_feat=False, add_inv_T=False)), \n",
    "        ('pre_processing', pre_process),\n",
    "        ('standard_scaler', StandardScaler())    \n",
    "    ])\n",
    "    \n",
    "    X_train_pped = prepare_data_pipeline.fit_transform(X_train_2)\n",
    "    X_test_pped = prepare_data_pipeline.transform(X_test_cpy)\n",
    "    X_val_pped = prepare_data_pipeline.transform(X_val)\n",
    "    \n",
    "    ann = keras.wrappers.scikit_learn.KerasRegressor(build_model(input_shape=X_train_pped.shape[1]))\n",
    "    \n",
    "    param_grid = {\n",
    "        \"n_hidden\": [1, 3, 5, 7],\n",
    "        \"n_neurons\":[10, 100, 1000],\n",
    "        \"learning_rate\":[1e-6, 1e-4, 1e-2],\n",
    "    }\n",
    "    \n",
    "    n_splits = n_splits\n",
    "    tscv = TimeSeriesSplit(n_splits)\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=tscv, n_jobs=-1, verbose=100,)\n",
    "    \n",
    "    # predicciones sobre el cojunto de validación\n",
    "    val_pred = model.predict(X_val_pped)\n",
    "    \n",
    "    # evaluación sobre el conjunto de test\n",
    "    predictions = model.predict(X_test_pped)\n",
    "\n",
    "    # build prediction matrix (ID,Production)\n",
    "    pred_matrix = np.stack((np.array(ID_test).astype(int), predictions.reshape(-1)), axis=-1)\n",
    "    df_pred = pd.DataFrame(data=pred_matrix.reshape(-1,2), columns=['ID','Production'])\n",
    "\n",
    "    # añadimos las predicciones al dataframe final que contendrá las de todas las WF\n",
    "    submission_df = submission_df.append(df_pred, ignore_index=True) \n",
    "    print('Validation CAPE:', metric.get_cape(y_val, val_pred.reshape(-1)))\n",
    "    print('Predictions for {} has been added to submission_df'.format(WF))\n",
    "    print('---------------')\n",
    "    \n",
    "# generamos fichero csv para el submission\n",
    "submission_df.to_csv(\"../../TFM/models/ANN.csv\", index=False, sep=\",\") \n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(np.isnan(X_test_pped))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import reciprocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(hist(reciprocal(1e-1,1e-2,1e-3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(ID_test).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 1: Regresión polinomial con regularización *ridge*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Automatización de pre-procesado, modelización, validación y prediccion\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.functions import metric\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model as lm\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = X_train['WF'].unique()\n",
    "\n",
    "# lista para guardar los modelos, uno por WF\n",
    "models = []\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "\n",
    "    # We'll add new columns NWPX_<met_var> without missing values\n",
    "    # new cols: NWP1_U, NWP1_V, NWP1_T, NWP2_U, NWP2_V, NWP3_U, NWP3_V, NWP3_T, NWP4_U, NWP4_V, NWP4_CLCT\n",
    "    \n",
    "    new_cols = ['NWP1_U','NWP1_V','NWP1_T','NWP2_U',\n",
    "                'NWP2_V','NWP3_U','NWP3_V','NWP3_T',\n",
    "                'NWP4_U','NWP4_V','NWP4_CLCT']\n",
    "\n",
    "    def add_new_cols(new_cols, df):\n",
    "        for col in new_cols:\n",
    "            df[col] = np.nan    \n",
    "            \n",
    "    add_new_cols(new_cols, X_train_cpy)\n",
    "    add_new_cols(new_cols, X_test_cpy)\n",
    "    \n",
    "    cols_train = X_train.columns[3:]\n",
    "    cols_test = X_test.columns[3:-9]\n",
    "    X_train_cpy = input_missing_values(X_train_cpy, cols_train)\n",
    "    X_test_cpy = input_missing_values(X_test_cpy, cols_test)\n",
    "    \n",
    "    col_list = ['NWP2_U','NWP2_V','NWP3_U','NWP3_V','NWP3_T']\n",
    "    X_train_cpy.index = X_train_cpy['Time']\n",
    "    \n",
    "    del X_train_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_train_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=2,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "\n",
    "    \n",
    "    X_train_cpy.reset_index(inplace=True)\n",
    "\n",
    "    X_test_cpy.index = X_test_cpy['Time']\n",
    "    \n",
    "    del X_test_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_test_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=100,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "    \n",
    "        \n",
    "    X_test_cpy.reset_index(inplace=True) \n",
    "\n",
    "    X_train_cpy['U'] = (X_train_cpy.NWP1_U + X_train_cpy.NWP2_U + X_train_cpy.NWP3_U + X_train_cpy.NWP4_U)/4\n",
    "    X_train_cpy['V'] = (X_train_cpy.NWP1_V + X_train_cpy.NWP2_V + X_train_cpy.NWP3_V + X_train_cpy.NWP4_V)/4\n",
    "    X_train_cpy['T'] = (X_train_cpy.NWP1_T + X_train_cpy.NWP3_T)/2\n",
    "    X_train_cpy['CLCT'] = (X_train_cpy.NWP4_CLCT)\n",
    "    X_train_cpy['T2'] = np.sqrt(X_train_cpy['T'])\n",
    "    \n",
    "    X_test_cpy['U'] = (X_test_cpy.NWP1_U + X_test_cpy.NWP2_U + X_test_cpy.NWP3_U)/3\n",
    "    X_test_cpy['V'] = (X_test_cpy.NWP1_V + X_test_cpy.NWP2_V + X_test_cpy.NWP3_V)/3\n",
    "    X_test_cpy['T'] = (X_test_cpy.NWP1_T + X_test_cpy.NWP3_T)/2\n",
    "    X_test_cpy['CLCT'] = (X_test_cpy.NWP4_CLCT)\n",
    "    X_test_cpy['T2'] = np.sqrt(X_test_cpy['T'])\n",
    "\n",
    "    \n",
    "    \n",
    "    X_train_cpy = X_train_cpy[['Time','U','V','T','CLCT','T2']]\n",
    "    X_test_cpy = X_test_cpy[['Time','U','V','T','CLCT','T2']]\n",
    "    \n",
    "    # Hay 11 valores perdidos en la columna CLCT en X_test_cpy. Los imputamos con la moda\n",
    "    X_test_cpy.fillna(method='bfill', limit=11, inplace=True)\n",
    "\n",
    "\n",
    "    # Pre-procesado de los datos\n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [('drop_columns', 'drop',\n",
    "                                                     ['Time','U','V','T','CLCT','T2'])]\n",
    "    )\n",
    "\n",
    "    # definir pipeline\n",
    "    prepare_data_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', dtr.DerivedAttributesAdder(add_time_feat=False, add_cyclic_feat=False, add_vel_pot=False)), \n",
    "        ('pre_processing', pre_process),\n",
    "        ('std_scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # aplicar pipeline\n",
    "    X_train_pped = prepare_data_pipeline.fit_transform(X_train_cpy)\n",
    "\n",
    "    # creamos nuestro scorer basado en el CAPE\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "\n",
    "    # Modelización: Regresión polinomial utilizando CV con regularización tipo Ride\n",
    "    poly_features = PolynomialFeatures(degree=3, include_bias=False)\n",
    "    X_train_poly = poly_features.fit_transform(X_train_pped)\n",
    "    \n",
    "    rreg = lm.RidgeCV(alphas=np.logspace(-4, -3, 3, 4), store_cv_values=True)\n",
    "    rreg.fit(X_train_poly, Y_train_cpy)\n",
    "    \n",
    "    # guardamos modelo\n",
    "    models.append(joblib.dump(rreg, WF + '_rreg'))\n",
    "\n",
    "    # evaluación sobre el conjunto de test\n",
    "    X_test_pped = prepare_data_pipeline.transform(X_test_cpy)\n",
    "    X_test_poly = poly_features.fit_transform(X_test_pped)\n",
    "    predictions = rreg.predict(X_test_poly)\n",
    "    \n",
    "    # generamos matriz de predicciones (ID,Production)\n",
    "    pred_matrix = np.stack((np.array(ID_test).astype(int), predictions), axis=-1)\n",
    "    df_pred = pd.DataFrame(data=pred_matrix, columns=['ID','Production'])\n",
    "\n",
    "    # añadimos las predicciones al dataframe final que contendrá las de todas las WF\n",
    "    submission_df = submission_df.append(df_pred, ignore_index=True) \n",
    "    print('Predictions for {} has been added to submission_df'.format(WF))\n",
    "    print('RMSE for {} is {}'.format(WF, np.mean(np.sqrt(rreg.cv_values_))))\n",
    "\n",
    "# generamos fichero csv para el submission\n",
    "submission_df.to_csv(\"../../TFM/models/submission_rreg.csv\", index=False, sep=\",\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_train_cpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_cpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 2: Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Automatización de pre-procesado, modelización, validación y prediccion\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.functions import metric\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = X_train['WF'].unique()\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "\n",
    "    # We'll add new columns NWPX_<met_var> without missing values\n",
    "    # new cols: NWP1_U, NWP1_V, NWP1_T, NWP2_U, NWP2_V, NWP3_U, NWP3_V, NWP3_T, NWP4_U, NWP4_V, NWP4_CLCT\n",
    "    \n",
    "    new_cols = ['NWP1_U','NWP1_V','NWP1_T','NWP2_U',\n",
    "                'NWP2_V','NWP3_U','NWP3_V','NWP3_T',\n",
    "                'NWP4_U','NWP4_V','NWP4_CLCT']\n",
    "\n",
    "    def add_new_cols(new_cols, df):\n",
    "        for col in new_cols:\n",
    "            df[col] = np.nan    \n",
    "            \n",
    "    add_new_cols(new_cols, X_train_cpy)\n",
    "    add_new_cols(new_cols, X_test_cpy)\n",
    "    \n",
    "    cols_train = X_train.columns[3:]\n",
    "    cols_test = X_test.columns[3:-9]\n",
    "    X_train_cpy = input_missing_values(X_train_cpy, cols_train)\n",
    "    X_test_cpy = input_missing_values(X_test_cpy, cols_test)\n",
    "    \n",
    "    col_list = ['NWP2_U','NWP2_V','NWP3_U','NWP3_V','NWP3_T']\n",
    "    X_train_cpy.index = X_train_cpy['Time']\n",
    "    \n",
    "    del X_train_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_train_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=2,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "        \n",
    "    X_train_cpy.reset_index(inplace=True)\n",
    "    \n",
    "    X_test_cpy.index = X_test_cpy['Time']\n",
    "    \n",
    "    del X_test_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_test_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=100,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "    \n",
    "        \n",
    "    X_test_cpy.reset_index(inplace=True) \n",
    "    \n",
    "\n",
    "    X_train_cpy['U'] = X_train_cpy.NWP1_U\n",
    "    X_train_cpy['V'] = X_train_cpy.NWP1_V\n",
    "    X_train_cpy['T'] = X_train_cpy.NWP3_T\n",
    "    X_train_cpy['CLCT'] = X_train_cpy.NWP4_CLCT\n",
    "    \n",
    "    X_test_cpy['U'] = X_test_cpy.NWP1_U\n",
    "    X_test_cpy['V'] = X_test_cpy.NWP1_V\n",
    "    X_test_cpy['T'] = X_test_cpy.NWP3_T\n",
    "    X_test_cpy['CLCT'] = X_test_cpy.NWP4_CLCT\n",
    " \n",
    "    X_train_cpy = X_train_cpy[['ID','Time','U','V','T','CLCT']]\n",
    "    X_test_cpy = X_test_cpy[['ID','Time','U','V','T','CLCT']]\n",
    "\n",
    "    ####### Limpieza de outliers ########\n",
    "    X_train_cpy.loc[X_train_cpy['CLCT'] < 0, 'CLCT'] = 0.0\n",
    "    X_test_cpy.loc[X_test_cpy['CLCT'] < 0, 'CLCT'] = 0.0\n",
    "    \n",
    "    # añadir columna Production\n",
    "    X_train_cpy['Production'] = list(Y_train_cpy['Production'])\n",
    "    \n",
    "    # calcular módulo velocidad del viento\n",
    "    X_train_cpy['vel'] = X_train_cpy.apply(get_wind_velmod, axis=1)\n",
    "    \n",
    "    X_ = X_train_cpy[['vel','Production']]\n",
    "    \n",
    "    # parámetros por granja\n",
    "    if WF == 'WF1':\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.25\n",
    "        frac_std = 0.85\n",
    "        threshold_type = 'scalar'\n",
    "        bottom_max = 6.\n",
    "    elif WF == \"WF2\":\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.15\n",
    "        frac_std = 0.8\n",
    "        threshold_type = 'scalar'\n",
    "        bottom_max = 5.5\n",
    "    elif WF == \"WF3\":\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.1\n",
    "        frac_std = 0.8\n",
    "        threshold_type = 'scalar'\n",
    "        bottom_max = 5.5\n",
    "    elif WF == \"WF4\":\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.1\n",
    "        frac_std = 0.95\n",
    "        threshold_type = 'scalar'\n",
    "        bottom_max = 5.3\n",
    "    elif WF == \"WF5\":\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.1\n",
    "        frac_std = 0.55\n",
    "        threshold_type = 'scalar'\n",
    "        bottom_max = 5.\n",
    "    else:\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.05\n",
    "        frac_std = 1.\n",
    "        threshold_type = 'std'\n",
    "        bottom_max = 5.\n",
    "\n",
    "        \n",
    "    # top-curve stacked outliers\n",
    "    top_stacked = filters.window_range_flag(\n",
    "        X_.Production, top_frac_max * X_.Production.max(), X_.Production.max(), X_.vel, 12.5, 2000.\n",
    "    )\n",
    "    \n",
    "    # sparse outliers\n",
    "    max_bin = 0.97*X_['Production'].max()\n",
    "    sparse_outliers = filters.bin_filter(\n",
    "        X_.Production, X_.vel, sparse_bin_width, frac_std * X_.Production.std(), 'median', 0.1, max_bin, threshold_type, 'all')\n",
    "    \n",
    "    # bottom-curve stacked outliers\n",
    "    bottom_stacked = filters.window_range_flag(X_.vel, bottom_max, 40, X_.Production, 0.05, 2000.)\n",
    "    \n",
    "    # deleting outliers\n",
    "    X_.vel = X_.vel[(~top_stacked) & (~sparse_outliers) & (~bottom_stacked)]\n",
    "    X_.Production = X_.Production[(~top_stacked) & (~sparse_outliers) & (~bottom_stacked)]\n",
    "    plot_flagged_pc(X_.vel, X_.Production, np.repeat('True', len(X_.vel)), 0.7)\n",
    "    \n",
    "    # seleccionar filas correspondientes a los no-outliers\n",
    "    X_train_cpy = X_train_cpy.loc[X_train_cpy['vel'].isin(X_.vel)]\n",
    "    Y_train_cpy = Y_train_cpy.loc[Y_train_cpy['ID'].isin(X_train_cpy['ID'])]\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "\n",
    "    \n",
    "    # Eliminamos las columnas 'vel' y 'Production'\n",
    "    del X_train_cpy['vel']\n",
    "    del X_train_cpy['Production']\n",
    "    \n",
    "    ###################################\n",
    "\n",
    "    # Pre-procesado de los datos\n",
    "    drop_lst = ['ID','Time','U','V','T','CLCT']\n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [('drop_columns', 'drop', drop_lst)])\n",
    "\n",
    "    # definir pipeline\n",
    "    prepare_data_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', dtr.NewFeaturesAdder(add_time_feat=False, add_cycl_feat=False, add_inv_T=False)), \n",
    "        ('pre_processing', pre_process),\n",
    "        # ('power_transf', PowerTransformer(method='yeo-johnson', standardize=True))\n",
    "    ])\n",
    "\n",
    "    # aplicar pipeline\n",
    "    X_train_pped = prepare_data_pipeline.fit_transform(X_train_cpy)\n",
    "    X_test_pped = prepare_data_pipeline.transform(X_test_cpy)\n",
    "\n",
    "    # feature_sel = True\n",
    "    # fe# ature_names = X_train_cpy.drop(drop_lst, axis=1).columns\n",
    "    # \n",
    "    # # Feature selection \n",
    "    # if feature_sel: \n",
    "    #     ## using Mutual Information\n",
    "    #     selec_k_best = SelectKBest(mutual_info_regression, k=7)\n",
    "    #     selec_k_best.fit(X_train_pped, Y_train_cpy)\n",
    "    #     X_train_pped = selec_k_best.transform(X_train_pped)\n",
    "    #     X_test_pped = selec_k_best.transform(X_test_pped)\n",
    "    # \n",
    "    #     mask =selec_k_best.get_support() #list of booleans\n",
    "    #     selected_feat = [] \n",
    "    # \n",
    "    #     for bool, feature in zip(mask, feature_names):\n",
    "    #         if bool:\n",
    "    #             selected_feat.append(feature) \n",
    "\n",
    "    # creamos nuestro scorer basado en el CAPE\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [75, 150, 225],\n",
    "        'max_features': ['sqrt', 'log2', 'auto'],\n",
    "        'max_depth': [10, 50, 100],\n",
    "        'min_samples_split': [15, 30, 45],\n",
    "        'min_samples_leaf': [4, 6, 8],\n",
    "    }\n",
    "    \n",
    "    \n",
    "    n_splits = 7 \n",
    "    forest_reg = RandomForestRegressor(bootstrap=True, random_state=42)\n",
    "    grid_search = GridSearchCV(\n",
    "        forest_reg, \n",
    "        param_grid, \n",
    "        cv=TimeSeriesSplit(n_splits),\n",
    "        scoring=cape_scorer,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train_pped, Y_train_cpy)\n",
    "    final_model = grid_search.best_estimator_\n",
    "    final_model.fit(X_train_pped, Y_train_cpy)\n",
    "\n",
    "    # evaluación sobre el conjunto de test\n",
    "    predictions = final_model.predict(X_test_pped)\n",
    "\n",
    "    # generamos matriz de predicciones (ID,Production)\n",
    "    pred_matrix = np.stack((np.array(ID_test).astype(int), predictions), axis=-1)\n",
    "    df_pred = pd.DataFrame(data=pred_matrix, columns=['ID','Production'])\n",
    "\n",
    "    # añadimos las predicciones al dataframe final que contendrá las de todas las WF\n",
    "    submission_df = submission_df.append(df_pred, ignore_index=True)  \n",
    "    print('Best CV score for {}: {}'.format(WF, -grid_search.best_score_))\n",
    "    print('Predictions for {} has been added to submission_df'.format(WF))\n",
    "    print('--------')\n",
    "\n",
    "# generamos fichero csv para el submission\n",
    "submission_df.to_csv(\"../../TFM/models/submission_RF.csv\", index=False, sep=\",\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 3: SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Automatización de pre-procesado, modelización, validación y prediccion\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.functions import metric\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "import hdbscan\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = X_train['WF'].unique()\n",
    "\n",
    "# lista para guardar los modelos, uno por WF\n",
    "models = []\n",
    "scores_list = []\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "\n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "\n",
    "    # We'll add new columns NWPX_<met_var> without missing values\n",
    "    # new cols: NWP1_U, NWP1_V, NWP1_T, NWP2_U, NWP2_V, NWP3_U, NWP3_V, NWP3_T, NWP4_U, NWP4_V, NWP4_CLCT\n",
    "    \n",
    "    new_cols = ['NWP1_U','NWP1_V','NWP1_T','NWP2_U',\n",
    "                'NWP2_V','NWP3_U','NWP3_V','NWP3_T',\n",
    "                'NWP4_U','NWP4_V','NWP4_CLCT']\n",
    "\n",
    "    def add_new_cols(new_cols, df):\n",
    "        for col in new_cols:\n",
    "            df[col] = np.nan    \n",
    "            \n",
    "    add_new_cols(new_cols, X_train_cpy)\n",
    "    add_new_cols(new_cols, X_test_cpy)\n",
    "    \n",
    "    cols_train = X_train.columns[3:]\n",
    "    cols_test = X_test.columns[3:-9]\n",
    "    X_train_cpy = input_missing_values(X_train_cpy, cols_train)\n",
    "    X_test_cpy = input_missing_values(X_test_cpy, cols_test)\n",
    "    \n",
    "    col_list = ['NWP2_U','NWP2_V','NWP3_U','NWP3_V','NWP3_T']\n",
    "    X_train_cpy.index = X_train_cpy['Time']\n",
    "    \n",
    "    del X_train_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_train_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=2,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "\n",
    "    \n",
    "    X_train_cpy.reset_index(inplace=True)\n",
    "\n",
    "    X_test_cpy.index = X_test_cpy['Time']\n",
    "    \n",
    "    del X_test_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_test_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=100,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "    \n",
    "        \n",
    "    X_test_cpy.reset_index(inplace=True) \n",
    "\n",
    "    X_train_cpy['U'] = X_train_cpy.NWP1_U\n",
    "    X_train_cpy['V'] = X_train_cpy.NWP1_V\n",
    "    X_train_cpy['T'] = X_train_cpy.NWP3_T\n",
    "    X_train_cpy['CLCT'] = X_train_cpy.NWP4_CLCT\n",
    "    \n",
    "    X_test_cpy['U'] = X_test_cpy.NWP1_U\n",
    "    X_test_cpy['V'] = X_test_cpy.NWP1_V\n",
    "    X_test_cpy['T'] = X_test_cpy.NWP3_T\n",
    "    X_test_cpy['CLCT'] = X_test_cpy.NWP4_CLCT\n",
    " \n",
    "    X_train_cpy = X_train_cpy[['ID','Time','U','V','T','CLCT']]\n",
    "    X_test_cpy = X_test_cpy[['ID','Time','U','V','T','CLCT']]\n",
    "    \n",
    "    # Hay 11 valores perdidos en la columna CLCT en X_test_cpy. Los imputamos con la moda\n",
    "    X_test_cpy.fillna(method='bfill', limit=11, inplace=True)\n",
    "    \n",
    "    \n",
    "    # valores negativos en CLCT\n",
    "    X_train_cpy.loc[X_train_cpy['CLCT'] < 0, 'CLCT'] = 0.0\n",
    "    X_test_cpy.loc[X_test_cpy['CLCT'] < 0, 'CLCT'] = 0.0\n",
    "\n",
    "    \n",
    "    # Pre-procesado de los datos\n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [('drop_columns', 'drop', \n",
    "                                                     ['ID','Time','U','V','CLCT','month'])]\n",
    "    )\n",
    "\n",
    "    # definir pipeline\n",
    "    prepare_data_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', dtr.NewFeaturesAdder(add_inv_T=False, add_cycl_feat=False)), \n",
    "        ('pre_processing', pre_process),\n",
    "        ('std_scaler', PowerTransformer(method='yeo-johnson', standardize=True))\n",
    "    ])\n",
    "\n",
    "    # aplicar pipeline\n",
    "    X_train_pped = prepare_data_pipeline.fit_transform(X_train_cpy)\n",
    "    \n",
    "    ####### Limpieza de outliers ########\n",
    "    \n",
    "    # añadir columna Production\n",
    "    X_train_cpy['Production'] = Y_train_cpy.to_list()\n",
    "    \n",
    "    # calcular módulo velocidad del viento\n",
    "    X_train_cpy['vel'] = X_train_cpy.apply(get_wind_velmod, axis=1)\n",
    "    \n",
    "    # formar matriz de datos \n",
    "    scaler = StandardScaler()\n",
    "    X1 = scaler.fit_transform(np.array(X_train_cpy['vel']).reshape(-1,1))\n",
    "    X2 = scaler.fit_transform(np.array(X_train_cpy['Production']).reshape(-1,1))\n",
    "    X = np.concatenate((X1,X2), axis=1)\n",
    "    \n",
    "    # algoritmo para detección de outliers\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=20, metric='mahalanobis', algorithm='brute').fit(X)\n",
    "    threshold = pd.Series(clusterer.outlier_scores_).quantile(0.95)\n",
    "    outliers = np.where(clusterer.outlier_scores_ > threshold)[0]\n",
    "    \n",
    "    # Eliminamos los registros outliers \n",
    "    X_train_pped = np.delete(X_train_pped, outliers, axis=0)\n",
    "    \n",
    "    # Eliminamos las observaciones corresp#ondientes de Y_train\n",
    "    Y_train_cpy = np.delete(X2, outliers, axis=0)\n",
    "    \n",
    "    # Eliminamos las columnas 'vel' y 'Production'\n",
    "    del X_train_cpy['vel']\n",
    "    del X_train_cpy['Production']\n",
    "    \n",
    "    ###################################\n",
    "    \n",
    "    '''\n",
    "    print('Nº features antes de la seleccion: ', X_train_pped.shape[1])\n",
    "    # seleccionar features más importantes mediante Random Forest\n",
    "    rf = RandomForestRegressor(random_state=42)\n",
    "    rf_reg = rf.fit(X_train_pped, Y_train_cpy)\n",
    "    sel = SelectFromModel(rf_reg, prefit=True, threshold='1.75*median')\n",
    "    X_train_pped = sel.transform(X_train_pped)\n",
    "    \n",
    "    print('Nº features después de la seleccion: ', X_train_pped.shape[1])\n",
    "    '''\n",
    "  \n",
    "\n",
    "    # creamos nuestro scorer basado en el CAPE\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "\n",
    "    # Entrenamiento del modelo mediante k-fold cross validation\n",
    "    # Búsqueda de hiperparámetros mediante Gridsearch \n",
    "    param_grid = {\n",
    "        'kernel': ('linear', 'rbf','poly'), \n",
    "        'C':[0.01, 1, 0.1, 10],\n",
    "        'gamma': [0.00001, 0.001, 1],\n",
    "        'epsilon':[0.1,0.3,0.5]\n",
    "    }\n",
    "\n",
    "    svm_reg = SVR()\n",
    "    grid_search_svm = GridSearchCV(\n",
    "        svm_reg, \n",
    "        param_grid, \n",
    "        cv= TimeSeriesSplit(n_splits=7),\n",
    "        scoring=cape_scorer,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    grid_search_svm.fit(X_train_pped, Y_train_cpy)\n",
    "\n",
    "    # Reentrenamos sin validación cruzada utilizando los mejores \n",
    "    # parámetros obtenidos con la validación cruzada\n",
    "    \n",
    "    svm_reg2 = SVR(kernel = grid_search_svm.best_params_['kernel'],\n",
    "                  C = grid_search_svm.best_params_['C'],\n",
    "                  gamma = grid_search_svm.best_params_['gamma'],\n",
    "                  epsilon = grid_search_svm.best_params_['epsilon'])\n",
    "    \n",
    "    ttreg = TransformedTargetRegressor(regressor=svm_reg2, \n",
    "                                       transformer=PowerTransformer(method='yeo-johnson', standardize=True), \n",
    "                                       check_inverse=False)\n",
    "\n",
    "    ttreg.fit(X_train_pped, Y_train_cpy)\n",
    "\n",
    "    # evaluación sobre el conjunto de test\n",
    "    X_test_pped = prepare_data_pipeline.transform(X_test_cpy)\n",
    "    # X_test_pped = sel.transform(X_test_pped)\n",
    "    predictions = ttreg.predict(X_test_pped)\n",
    "    \n",
    "    # generamos matriz de predicciones (ID,Production)\n",
    "    pred_matrix = np.stack((np.array(ID_test).astype(int), predictions), axis=-1)\n",
    "    df_pred = pd.DataFrame(data=pred_matrix, columns=['ID','Production'])\n",
    "\n",
    "    # añadimos las predicciones al dataframe final que contendrá las de todas las WF\n",
    "    submission_df = submission_df.append(df_pred, ignore_index=True) \n",
    "    print('R2 for {} is {}'.format(WF, ttreg.score(X_train_pped, Y_train_cpy)))\n",
    "    print('Predictions for {} has been added to submission_df'.format(WF))\n",
    "    \n",
    "    scores_list.append(ttreg.score(X_train_pped, Y_train_cpy))\n",
    "    \n",
    "global_score = np.mean(scores_list)\n",
    "\n",
    "print('********************************************')\n",
    "print('Global score: ', global_score)\n",
    "print('********************************************')\n",
    "\n",
    "\n",
    "# generamos fichero csv para el submission\n",
    "submission_df.to_csv(\"../../TFM/models/submission_SVR.csv\", index=False, sep=\",\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate 3 plots: the test and training learning curve, the training\n",
    "    samples vs fit times curve, the fit times vs score curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    axes : array of 3 axes, optional (default=None)\n",
    "        Axes to use for plotting the curves.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 5-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - :term:`CV splitter`,\n",
    "          - An iterable yielding (train, test) splits as arrays of indices.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : int or None, optional (default=None)\n",
    "        Number of jobs to run in parallel.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
    "        Relative or absolute numbers of training examples that will be used to\n",
    "        generate the learning curve. If the dtype is float, it is regarded as a\n",
    "        fraction of the maximum size of the training set (that is determined\n",
    "        by the selected validation method), i.e. it has to be within (0, 1].\n",
    "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
    "        Note that for classification the number of samples usually have to\n",
    "        be big enough to contain at least one sample from each class.\n",
    "        (default: np.linspace(0.1, 1.0, 5))\n",
    "    \"\"\"\n",
    "    if axes is None:\n",
    "        _, axes = plt.subplots(1,3, figsize=(20, 5))\n",
    "\n",
    "    axes[0].set_title(title)\n",
    "    if ylim is not None:\n",
    "        axes[0].set_ylim(*ylim)\n",
    "    axes[0].set_xlabel(\"Training examples\")\n",
    "    axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = \\\n",
    "        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
    "                       train_sizes=train_sizes,\n",
    "                       return_times=True)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes[0].grid()\n",
    "    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                         color=\"r\")\n",
    "    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
    "                         color=\"g\")\n",
    "    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "                 label=\"Training score\")\n",
    "    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "                 label=\"Cross-validation score\")\n",
    "    axes[0].legend(loc=\"best\")\n",
    "\n",
    "    # Plot n_samples vs fit_times\n",
    "    axes[1].grid()\n",
    "    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n",
    "    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n",
    "                         fit_times_mean + fit_times_std, alpha=0.1)\n",
    "    axes[1].set_xlabel(\"Training examples\")\n",
    "    axes[1].set_ylabel(\"fit_times\")\n",
    "    axes[1].set_title(\"Scalability of the model\")\n",
    "\n",
    "    # Plot fit_time vs score\n",
    "    axes[2].grid()\n",
    "    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n",
    "    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1)\n",
    "    axes[2].set_xlabel(\"fit_times\")\n",
    "    axes[2].set_ylabel(\"Score\")\n",
    "    axes[2].set_title(\"Performance of the model\")\n",
    "\n",
    "    return plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 4: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Automatización de pre-procesado, modelización, validación y prediccion\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.functions import metric\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, learning_curve\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import hdbscan\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = X_train['WF'].unique()\n",
    "\n",
    "# lista para guardar los modelos, uno por WF\n",
    "models = []\n",
    "scores_list = []\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "\n",
    "    # We'll add new columns NWPX_<met_var> without missing values\n",
    "    # new cols: NWP1_U, NWP1_V, NWP1_T, NWP2_U, NWP2_V, NWP3_U, NWP3_V, NWP3_T, NWP4_U, NWP4_V, NWP4_CLCT\n",
    "    \n",
    "    new_cols = ['NWP1_U','NWP1_V','NWP1_T','NWP2_U',\n",
    "                'NWP2_V','NWP3_U','NWP3_V','NWP3_T',\n",
    "                'NWP4_U','NWP4_V','NWP4_CLCT']\n",
    "\n",
    "    def add_new_cols(new_cols, df):\n",
    "        for col in new_cols:\n",
    "            df[col] = np.nan    \n",
    "            \n",
    "    add_new_cols(new_cols, X_train_cpy)\n",
    "    add_new_cols(new_cols, X_test_cpy)\n",
    "    \n",
    "    cols_train = X_train.columns[3:]\n",
    "    cols_test = X_test.columns[3:-9]\n",
    "    X_train_cpy = input_missing_values(X_train_cpy, cols_train)\n",
    "    X_test_cpy = input_missing_values(X_test_cpy, cols_test)\n",
    "    \n",
    "    col_list = ['NWP2_U','NWP2_V','NWP3_U','NWP3_V','NWP3_T']\n",
    "    X_train_cpy.index = X_train_cpy['Time']\n",
    "    \n",
    "    del X_train_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_train_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=2,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "\n",
    "    \n",
    "    X_train_cpy.reset_index(inplace=True)\n",
    "\n",
    "    X_test_cpy.index = X_test_cpy['Time']\n",
    "    \n",
    "    del X_test_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_test_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=100,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "    \n",
    "        \n",
    "    X_test_cpy.reset_index(inplace=True) \n",
    "\n",
    "    X_train_cpy['U'] = (X_train_cpy.NWP1_U + X_train_cpy.NWP2_U + X_train_cpy.NWP3_U + X_train_cpy.NWP4_U)/4\n",
    "    X_train_cpy['V'] = (X_train_cpy.NWP1_V + X_train_cpy.NWP2_V + X_train_cpy.NWP3_V + X_train_cpy.NWP4_V)/4\n",
    "    X_train_cpy['T'] = (X_train_cpy.NWP1_T + X_train_cpy.NWP3_T)/2\n",
    "    X_train_cpy['CLCT'] = (X_train_cpy.NWP4_CLCT)\n",
    "    \n",
    "    X_test_cpy['U'] = (X_test_cpy.NWP1_U + X_test_cpy.NWP2_U + X_test_cpy.NWP3_U)/3\n",
    "    X_test_cpy['V'] = (X_test_cpy.NWP1_V + X_test_cpy.NWP2_V + X_test_cpy.NWP3_V)/3\n",
    "    X_test_cpy['T'] = (X_test_cpy.NWP1_T + X_test_cpy.NWP3_T)/2\n",
    "    X_test_cpy['CLCT'] = (X_test_cpy.NWP4_CLCT)\n",
    "\n",
    "    \n",
    "    X_train_cpy = X_train_cpy[['ID','Time','U','V','T','CLCT']]\n",
    "    X_test_cpy = X_test_cpy[['ID','Time','U','V','T','CLCT']]\n",
    "    \n",
    "    # Hay 11 valores perdidos en la columna CLCT en X_test_cpy. Los imputamos\n",
    "    X_test_cpy['CLCT'].fillna(method='bfill', limit=11, inplace=True)\n",
    "    \n",
    "     ####### Limpiar outliers y valores anómalos #######\n",
    "    \n",
    "    # valores negativos en CLCT\n",
    "    X_train_cpy.loc[X_train_cpy['CLCT'] < 0, 'CLCT'] = 0.0\n",
    "    X_test_cpy.loc[X_test_cpy['CLCT'] < 0, 'CLCT'] = 0.0\n",
    "\n",
    "    # añadir columna Production\n",
    "    X_train_cpy['Production'] = Y_train_cpy.to_list()\n",
    "    \n",
    "    # calcular módulo velocidad del viento\n",
    "    X_train_cpy['vel'] = X_train_cpy.apply(get_wind_velmod, axis=1)\n",
    "    \n",
    "    # formar matriz de datos \n",
    "    mm = PowerTransformer()\n",
    "\n",
    "    X1 = mm.fit_transform(np.array(X_train_cpy['vel']).reshape(-1,1))\n",
    "    X2 = mm.fit_transform(np.array(X_train_cpy['Production']).reshape(-1,1))\n",
    "    X = np.concatenate((X1,X2), axis=1)\n",
    "    \n",
    "    # algoritmo para detección de outliers\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=20).fit(X)\n",
    "    threshold = pd.Series(clusterer.outlier_scores_).quantile(0.96)\n",
    "    outliers = np.where(clusterer.outlier_scores_ > threshold)[0]\n",
    "    \n",
    "    # Eliminamos los registros outliers \n",
    "    X_train_cpy.drop(X_train_cpy.index[list(outliers)], inplace=True)\n",
    "    \n",
    "    # Eliminamos las observaciones corresp#ondientes de Y_train\n",
    "    Y_train_cpy = Y_train_cpy.loc[X_train_cpy['ID'].values - 1]\n",
    "    \n",
    "    # Eliminamos las columnas 'vel' y 'Production'\n",
    "    del X_train_cpy['vel']\n",
    "    del X_train_cpy['Production']\n",
    "    \n",
    "    ###################################\n",
    "\n",
    "    # Pre-procesado de los datos\n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [('drop_columns', 'drop', ['ID','Time','U','V','T',\n",
    "                                                                             'month','hour','w_dir'])]\n",
    "    )\n",
    "\n",
    "    # definir pipeline\n",
    "    prepare_data_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', dtr.NewFeaturesAdder()), \n",
    "        ('pre_processing', pre_process)\n",
    "    ])\n",
    "\n",
    "\n",
    "    # aplicar pipeline\n",
    "    X_train_pped = prepare_data_pipeline.fit_transform(X_train_cpy, Y_train_cpy)\n",
    "    # print('Nº features antes de la seleccion: ', X_train_pped.shape[1])\n",
    "    \n",
    "    # seleccionar features más importantes mediante Random Forest\n",
    "    #rf = RandomForestRegressor(random_state=42)\n",
    "    #rf_reg = rf.fit(X_train_pped, Y_train_cpy)\n",
    "    #sel = SelectFromModel(rf_reg, prefit=True, threshold='2*median')\n",
    "    #X_train_pped = sel.transform(X_train_pped)\n",
    "    \n",
    "    # print('Nº features después de la seleccion: ', X_train_pped.shape[1])               \n",
    "\n",
    "\n",
    "    # creamos nuestro scorer basado en el CAPE\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "\n",
    "    # Modelización: Random Forest\n",
    "    # Entrenamiento del modelo mediante k-fold cross validation\n",
    "    # Búsqueda de hiperparámetros mediante Gridsearch \n",
    "    \n",
    "    param_grid = {   \n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5],\n",
    "    }\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    xgb_reg = xgb.XGBRegressor(objective ='reg:squarederror')\n",
    "    grid_search_xgb = GridSearchCV(\n",
    "        xgb_reg, \n",
    "        param_grid, \n",
    "        cv=tscv,\n",
    "        scoring=cape_scorer,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    grid_search_xgb.fit(X_train_pped, Y_train_cpy)\n",
    "    \n",
    "    reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = grid_search_xgb.best_params_['colsample_bytree'],\n",
    "                           gamma = grid_search_xgb.best_params_['gamma'],\n",
    "                           max_depth = grid_search_xgb.best_params_['max_depth'],\n",
    "                           min_child_weight = grid_search_xgb.best_params_['min_child_weight'],\n",
    "                           subsample = grid_search_xgb.best_params_['subsample'],\n",
    "                           random_state=42)\n",
    "                                       \n",
    "    reg.fit(X_train_pped, Y_train_cpy)\n",
    "    \n",
    "    # evaluación sobre el conjunto de test\n",
    "    X_test_pped = prepare_data_pipeline.transform(X_test_cpy)\n",
    "    # X_test_pped = sel.transform(X_test_pped)\n",
    "    predictions = reg.predict(X_test_pped)\n",
    "    \n",
    "    # generamos matriz de predicciones (ID,Production)\n",
    "    pred_matrix = np.stack((np.array(ID_test).astype(int), predictions), axis=-1)\n",
    "    df_pred = pd.DataFrame(data=pred_matrix, columns=['ID','Production'])\n",
    "\n",
    "    # añadimos las predicciones al dataframe final que contendrá las de todas las WF\n",
    "    submission_df = submission_df.append(df_pred, ignore_index=True) \n",
    "    print('R2 for {} is {}'.format(WF, reg.score(X_train_pped, Y_train_cpy)))\n",
    "    print('Predictions for {} has been added to submission_df'.format(WF))\n",
    "    \n",
    "    scores_list.append(reg.score(X_train_pped, Y_train_cpy))\n",
    "\n",
    "    title = \"Learning Curves\"\n",
    "    cv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "    estimator = reg\n",
    "    plot_learning_curve(estimator, title, X_train_pped, Y_train_cpy, cv=cv, n_jobs=-1)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "global_score = np.mean(scores_list)\n",
    "\n",
    "print('********************************************')\n",
    "print('Global score: ', global_score)\n",
    "print('********************************************')\n",
    "\n",
    "\n",
    "# generamos fichero csv para el submission\n",
    "submission_df.to_csv(\"../../TFM/models/submission_xgb2.csv\", index=False, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_cpy.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 5: Random Forest con validación Randomized Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "# Automatización de pre-procesado, modelización, validación y prediccion\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.functions import metric\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = X_train['WF'].unique()\n",
    "\n",
    "# lista para guardar los modelos, uno por WF\n",
    "models = []\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "\n",
    "    # We'll add new columns NWPX_<met_var> without missing values\n",
    "    # new cols: NWP1_U, NWP1_V, NWP1_T, NWP2_U, NWP2_V, NWP3_U, NWP3_V, NWP3_T, NWP4_U, NWP4_V, NWP4_CLCT\n",
    "    \n",
    "    new_cols = ['NWP1_U','NWP1_V','NWP1_T','NWP2_U',\n",
    "                'NWP2_V','NWP3_U','NWP3_V','NWP3_T',\n",
    "                'NWP4_U','NWP4_V','NWP4_CLCT']\n",
    "\n",
    "    def add_new_cols(new_cols, df):\n",
    "        for col in new_cols:\n",
    "            df[col] = np.nan    \n",
    "            \n",
    "    add_new_cols(new_cols, X_train_cpy)\n",
    "    add_new_cols(new_cols, X_test_cpy)\n",
    "    \n",
    "    cols_train = X_train.columns[3:]\n",
    "    cols_test = X_test.columns[3:-9]\n",
    "    X_train_cpy = input_missing_values(X_train_cpy, cols_train)\n",
    "    X_test_cpy = input_missing_values(X_test_cpy, cols_test)\n",
    "    \n",
    "    col_list = ['NWP2_U','NWP2_V','NWP3_U','NWP3_V','NWP3_T']\n",
    "    X_train_cpy.index = X_train_cpy['Time']\n",
    "    \n",
    "    del X_train_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_train_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=2,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "\n",
    "    \n",
    "    X_train_cpy.reset_index(inplace=True)\n",
    "\n",
    "    X_test_cpy.index = X_test_cpy['Time']\n",
    "    \n",
    "    del X_test_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_test_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=100,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "    \n",
    "        \n",
    "    X_test_cpy.reset_index(inplace=True) \n",
    "\n",
    "    X_train_cpy['U'] = (X_train_cpy.NWP1_U + X_train_cpy.NWP2_U + X_train_cpy.NWP3_U + X_train_cpy.NWP4_U)/4\n",
    "    X_train_cpy['V'] = (X_train_cpy.NWP1_V + X_train_cpy.NWP2_V + X_train_cpy.NWP3_V + X_train_cpy.NWP4_V)/4\n",
    "    X_train_cpy['T'] = (X_train_cpy.NWP1_T + X_train_cpy.NWP3_T)/2\n",
    "    X_train_cpy['CLCT'] = (X_train_cpy.NWP4_CLCT)\n",
    "    X_train_cpy['T2'] = np.sqrt(X_train_cpy['T'])\n",
    "    \n",
    "    X_test_cpy['U'] = (X_test_cpy.NWP1_U + X_test_cpy.NWP2_U + X_test_cpy.NWP3_U)/3\n",
    "    X_test_cpy['V'] = (X_test_cpy.NWP1_V + X_test_cpy.NWP2_V + X_test_cpy.NWP3_V)/3\n",
    "    X_test_cpy['T'] = (X_test_cpy.NWP1_T + X_test_cpy.NWP3_T)/2\n",
    "    X_test_cpy['CLCT'] = (X_test_cpy.NWP4_CLCT)\n",
    "    X_test_cpy['T2'] = np.sqrt(X_test_cpy['T'])\n",
    "\n",
    "    \n",
    "    X_train_cpy = X_train_cpy[['ID','Time','U','V','T','CLCT','T2']]\n",
    "    X_test_cpy = X_test_cpy[['ID','Time','U','V','T','CLCT','T2']]\n",
    "    \n",
    "    # Hay 11 valores perdidos en la columna CLCT en X_test_cpy. Los imputamos con la moda\n",
    "    X_test_cpy.fillna(method='bfill', limit=11, inplace=True)\n",
    "    \n",
    "    ####### Limpiar outliers #######\n",
    "    \n",
    "    # añadir columna Production\n",
    "    X_train_cpy['Production'] = Y_train_cpy.to_list()\n",
    "    \n",
    "    # calcular módulo velocidad del viento\n",
    "    X_train_cpy['vel'] = X_train_cpy.apply(get_wind_velmod, axis=1)\n",
    "    \n",
    "    # formar matriz de datos \n",
    "    X1 = X_train_cpy['vel'].values.reshape(-1,1)\n",
    "    X2 = X_train_cpy['Production'].values.reshape(-1,1)\n",
    "    X = np.concatenate((X1,X2), axis=1)\n",
    "    \n",
    "    # Definir el clasificador de outliers\n",
    "    clf = OneClassSVM(nu=0.17, gamma=0.06)\n",
    "    clf.fit(X)\n",
    "    \n",
    "    # Predección de outlier o inlier para cada punto\n",
    "    y_pred = clf.predict(X)\n",
    "    \n",
    "    # Añadirmos la columna 'oulier' con la predicción \n",
    "    X_train_cpy['outlier'] = y_pred.tolist()\n",
    "    \n",
    "    # Eliminamos los registros outliers \n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['outlier'] != -1]\n",
    "    \n",
    "    # Eliminamos las observaciones correspondientes de Y_train\n",
    "    Y_train_cpy = Y_train_cpy.loc[X_train_cpy['ID'].values - 1]\n",
    "    \n",
    "    # Eliminamos las columnas 'vel', 'outlier' y 'Production'\n",
    "    del X_train_cpy['vel']\n",
    "    del X_train_cpy['outlier']\n",
    "    del X_train_cpy['Production']\n",
    "    \n",
    "    ###################################\n",
    "\n",
    "    # Pre-procesado de los datos\n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [('drop_columns', 'drop', \n",
    "                                                     ['ID','Time','U','V','T2'])]\n",
    "    )\n",
    "\n",
    "    # definir pipeline\n",
    "    prepare_data_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', dtr.NewFeaturesAdder(add_w_shear=False, add_time_feat=False)), \n",
    "        ('pre_processing', pre_process),\n",
    "        ('std_scaler', StandardScaler())\n",
    "    ])\n",
    "    # aplciar pipeline\n",
    "    X_train_pped = prepare_data_pipeline.fit_transform(X_train_cpy)\n",
    "\n",
    "    # creamos nuestro scorer basado en el CAPE\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "\n",
    "    # Modelización: Random Forest\n",
    "    # Entrenamiento del modelo mediante k-fold cross validation\n",
    "    # Búsqueda de hiperparámetros mediante Randomized Search \n",
    "\n",
    "    forest_reg = RandomForestRegressor()\n",
    "\n",
    "    rf_random = RandomizedSearchCV(\n",
    "        estimator = forest_reg, \n",
    "        param_distributions = random_grid, \n",
    "        n_iter = 100, \n",
    "        cv = 5, \n",
    "        random_state=42, \n",
    "        scoring = cape_scorer,\n",
    "        n_jobs = -1\n",
    "    )\n",
    "\n",
    "    rf_random.fit(X_train_pped, Y_train_cpy)\n",
    "    final_model = rf_random.best_estimator_\n",
    "\n",
    "    # guardamos modelo\n",
    "    models.append(joblib.dump(final_model, WF + 'rfrand'))\n",
    "\n",
    "    # evaluación sobre el conjunto de test\n",
    "    X_test_pped = prepare_data_pipeline.transform(X_test_cpy)\n",
    "    predictions = final_model.predict(X_test_pped)\n",
    "\n",
    "    # generamos matriz de predicciones (ID,Production)\n",
    "    pred_matrix = np.stack((np.array(ID_test).astype(int), predictions), axis=-1)\n",
    "    df_pred = pd.DataFrame(data=pred_matrix, columns=['ID','Production'])\n",
    "\n",
    "    # añadimos las predicciones al dataframe final que contendrá las de todas las WF\n",
    "    submission_df = submission_df.append(df_pred, ignore_index=True)  \n",
    "    print('Best score for {}: {}'.format(WF, -rf_random.best_score_))\n",
    "    print('Predictions for {} has been added to submission_df'.format(WF))\n",
    "    print('--------')\n",
    "\n",
    "\n",
    "# generamos fichero csv para el submission\n",
    "submission_df.to_csv(\"./Data/submission_rfrand.csv\", index=False, sep=\",\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 6: Regresión con Elastic Net (Ridge + Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Automatización de pre-procesado, modelización, validación y prediccion\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor \n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.functions import metric\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.externals import joblib\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = X_train['WF'].unique()\n",
    "\n",
    "# lista para guardar los modelos, uno por WF\n",
    "# models = []\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "\n",
    "    # We'll add new columns NWPX_<met_var> without missing values\n",
    "    # new cols: NWP1_U, NWP1_V, NWP1_T, NWP2_U, NWP2_V, NWP3_U, NWP3_V, NWP3_T, NWP4_U, NWP4_V, NWP4_CLCT\n",
    "    \n",
    "    new_cols = ['NWP1_U','NWP1_V','NWP1_T','NWP2_U',\n",
    "                'NWP2_V','NWP3_U','NWP3_V','NWP3_T',\n",
    "                'NWP4_U','NWP4_V','NWP4_CLCT']\n",
    "\n",
    "    def add_new_cols(new_cols, df):\n",
    "        for col in new_cols:\n",
    "            df[col] = np.nan    \n",
    "            \n",
    "    add_new_cols(new_cols, X_train_cpy)\n",
    "    add_new_cols(new_cols, X_test_cpy)\n",
    "    \n",
    "    cols_train = X_train.columns[3:]\n",
    "    cols_test = X_test.columns[3:-9]\n",
    "    X_train_cpy = input_missing_values(X_train_cpy, cols_train)\n",
    "    X_test_cpy = input_missing_values(X_test_cpy, cols_test)\n",
    "    \n",
    "    col_list = ['NWP2_U','NWP2_V','NWP3_U','NWP3_V','NWP3_T']\n",
    "    X_train_cpy.index = X_train_cpy['Time']\n",
    "    \n",
    "    del X_train_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_train_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=2,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "\n",
    "    \n",
    "    X_train_cpy.reset_index(inplace=True)\n",
    "\n",
    "    X_test_cpy.index = X_test_cpy['Time']\n",
    "    \n",
    "    del X_test_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_test_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=100,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "    \n",
    "        \n",
    "    X_test_cpy.reset_index(inplace=True) \n",
    "\n",
    "    X_train_cpy['U'] = X_train_cpy.NWP1_U\n",
    "    X_train_cpy['V'] = X_train_cpy.NWP1_V\n",
    "    X_train_cpy['T'] = X_train_cpy.NWP3_T\n",
    "    X_train_cpy['CLCT'] = X_train_cpy.NWP4_CLCT\n",
    "    \n",
    "    X_test_cpy['U'] = X_test_cpy.NWP1_U\n",
    "    X_test_cpy['V'] = X_test_cpy.NWP1_V\n",
    "    X_test_cpy['T'] = X_test_cpy.NWP3_T\n",
    "    X_test_cpy['CLCT'] = X_test_cpy.NWP4_CLCT\n",
    " \n",
    "    X_train_cpy = X_train_cpy[['ID','Time','U','V','T','CLCT']]\n",
    "    X_test_cpy = X_test_cpy[['ID','Time','U','V','T','CLCT']]\n",
    "    \n",
    "    # Hay 11 valores perdidos en la columna CLCT en X_test_cpy. Los imputamos\n",
    "    X_test_cpy.fillna(method='bfill', limit=11, inplace=True)\n",
    "\n",
    "    ####### Limpiar outliers #######\n",
    "    # Negative values in CLCT\n",
    "    X_train_cpy.loc[X_train_cpy['CLCT'] < 0, 'CLCT'] = 0.0\n",
    "    X_test_cpy.loc[X_test_cpy['CLCT'] < 0, 'CLCT'] = 0.0\n",
    "    \n",
    "    # Locating and removing outliers based on wind power curve\n",
    "    # Add 'Production' column\n",
    "    X_train_cpy['Production'] = Y_train_cpy.to_list()\n",
    "    \n",
    "    # Calculate wind velocity module\n",
    "    X_train_cpy['vel'] = X_train_cpy.apply(dtr.get_wind_velmod, axis=1)\n",
    "    \n",
    "    # Build data matrix\n",
    "    mm = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "    \n",
    "    X1 = mm.fit_transform(X_train_cpy['vel'].values.reshape(-1,1))\n",
    "    X2 = mm.fit_transform(X_train_cpy['Production'].values.reshape(-1,1))\n",
    "    X = np.concatenate((X1,X2), axis=1)\n",
    "    \n",
    "    # Using DBSCAN to find outliers\n",
    "    from sklearn.cluster import DBSCAN\n",
    "    outlier_detection = DBSCAN(\n",
    "        eps = 0.1,\n",
    "        metric=\"l1\",\n",
    "        min_samples = 10,\n",
    "        n_jobs = -1)\n",
    "    \n",
    "    clusters = outlier_detection.fit_predict(X)\n",
    "    outliers = np.where(clusters == -1)[0]\n",
    "\n",
    "    # Deleting columns 'vel' and 'Production'\n",
    "    del X_train_cpy['vel']\n",
    "    del X_train_cpy['Production']\n",
    "    ###################################\n",
    "\n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [('drop_columns', 'drop', \n",
    "                                                     ['ID','Time','U','V',\n",
    "                                                      'month_sin','month_cos',\n",
    "                                                      'w_dir','month','hour'\n",
    "                                                      ])]\n",
    "    )\n",
    "    \n",
    "    prepare_data_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', dtr.NewFeaturesAdder(add_cycl_feat=True, add_time_feat=True)), \n",
    "        ('pre_processing', pre_process),\n",
    "        ('power_transf', PowerTransformer())\n",
    "    ])\n",
    "\n",
    "    # aplicar pipeline\n",
    "    X_train_pped = prepare_data_pipeline.fit_transform(X_train_cpy)\n",
    "    \n",
    "    # Deleting outlier related observations\n",
    "    X_train_pped = np.delete(X_train_pped, tuple(outliers), axis=0)\n",
    "\n",
    "    # Deleting the corresponding observations in y_train\n",
    "    Y_train_cpy = np.delete(Y_train_cpy.to_numpy(), list(outliers))\n",
    "    \n",
    "    # creamos nuestro scorer basado en el CAPE\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "\n",
    "    # Modelización: Regresión polinomial con regularización Elastic Net\n",
    "    # Entrenamiento del modelo mediante k-fold cross validation\n",
    "    # Búsqueda de hiperparámetros mediante Gridsearch \n",
    "    \n",
    "    poly_features = PolynomialFeatures(degree=3, include_bias=False)\n",
    "    X_train_poly = poly_features.fit_transform(X_train_pped)\n",
    "    \n",
    "    param_grid = [{\n",
    "        'alpha'     : np.logspace(-3, -2, 1, 2, 3),\n",
    "        'l1_ratio'  : [0.00, 0.25, 0.50, 0.75, 1.0],\n",
    "        'tol'       : [0.00001, 0.0001, 0.001]\n",
    "    }]\n",
    "    \n",
    "                                               \n",
    "    eNet = ElasticNet(selection='random')\n",
    "    grid_search = GridSearchCV(\n",
    "        eNet, \n",
    "        param_grid, \n",
    "        cv = TimeSeriesSplit(n_splits=7),\n",
    "        scoring=cape_scorer,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train_poly, Y_train_cpy)\n",
    "    \n",
    "    # Calculate cross validation scores\n",
    "    cvres = grid_search.cv_results_\n",
    "    print('Best CV score:', -grid_search.best_score_)\n",
    "\n",
    "    # Re-training without CV, using the best param#eters obtained by CV\n",
    "    eNet2 = ElasticNet(selection='random',\n",
    "                       alpha=grid_search.best_params_['alpha'],\n",
    "                       l1_ratio=grid_search.best_params_['l1_ratio'],\n",
    "                       tol=grid_search.best_params_['tol']\n",
    "                      )\n",
    "    \n",
    "    \n",
    "    ttreg = TransformedTargetRegressor(regressor=eNet2, \n",
    "                                       transformer=PowerTransformer(), \n",
    "                                       check_inverse=False)\n",
    "    \n",
    "    ttreg.fit(X_train_poly, Y_train_cpy)\n",
    "            \n",
    "    # evaluación sobre el conjunto de test\n",
    "    X_test_pped = prepare_data_pipeline.transform(X_test_cpy)\n",
    "    X_test_poly = poly_features.fit_transform(X_test_pped)\n",
    "    predictions = ttreg.predict(X_test_poly)\n",
    "    \n",
    "    # build prediction matrix (ID,Production)\n",
    "    pred_matrix = np.stack((np.array(ID_test).astype(int), predictions), axis=-1)\n",
    "    df_pred = pd.DataFrame(data=pred_matrix, columns=['ID','Production'])\n",
    "    \n",
    "    # añadimos las predicciones al dataframe final que contendrá las de todas las WF\n",
    "    submission_df = submission_df.append(df_pred, ignore_index=True) \n",
    "    # print('R^2 for {} is {}'.format(WF, ttreg.score(X_train_poly, Y_train_cpy)))\n",
    "    print('Predictions for {} has been added to submission_df'.format(WF))\n",
    "    print('---------------')\n",
    "\n",
    "# generamos fichero csv para el submission\n",
    "submission_df.to_csv(\"../../TFM/models/elasticNet_2.csv\", index=False, sep=\",\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 7: LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Automatización de pre-procesado, modelización, validación y prediccion\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.functions import metric\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model as lm\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = X_train['WF'].unique()\n",
    "\n",
    "# lista para guardar los modelos, uno por WF\n",
    "models = []\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "\n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "\n",
    "    # We'll add new columns NWPX_<met_var> without missing values\n",
    "    # new cols: NWP1_U, NWP1_V, NWP1_T, NWP2_U, NWP2_V, NWP3_U, NWP3_V, NWP3_T, NWP4_U, NWP4_V, NWP4_CLCT\n",
    "    \n",
    "    new_cols = ['NWP1_U','NWP1_V','NWP1_T','NWP2_U',\n",
    "                'NWP2_V','NWP3_U','NWP3_V','NWP3_T',\n",
    "                'NWP4_U','NWP4_V','NWP4_CLCT']\n",
    "\n",
    "    def add_new_cols(new_cols, df):\n",
    "        for col in new_cols:\n",
    "            df[col] = np.nan    \n",
    "            \n",
    "    add_new_cols(new_cols, X_train_cpy)\n",
    "    add_new_cols(new_cols, X_test_cpy)\n",
    "    \n",
    "    cols_train = X_train_cpy.columns[3:-11]\n",
    "    cols_test = X_test_cpy.columns[3:-11]\n",
    "    X_train_cpy = input_missing_values(X_train_cpy, cols_train)\n",
    "    X_test_cpy = input_missing_values(X_test_cpy, cols_train)\n",
    "    \n",
    "    col_list = ['NWP2_U','NWP2_V','NWP3_U','NWP3_V','NWP3_T']\n",
    "    X_train_cpy.index = X_train_cpy['Time']\n",
    "    \n",
    "    del X_train_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_train_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=2,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "\n",
    "    \n",
    "    X_train_cpy.reset_index(inplace=True)\n",
    "\n",
    "    X_test_cpy.index = X_test_cpy['Time']\n",
    "    \n",
    "    del X_test_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_test_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=100,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "    \n",
    "        \n",
    "    X_test_cpy.reset_index(inplace=True) \n",
    "\n",
    "    X_train_cpy['U'] = (X_train_cpy.NWP1_U + X_train_cpy.NWP2_U + X_train_cpy.NWP3_U + X_train_cpy.NWP4_U)/4\n",
    "    X_train_cpy['V'] = (X_train_cpy.NWP1_V + X_train_cpy.NWP2_V + X_train_cpy.NWP3_V + X_train_cpy.NWP4_V)/4\n",
    "    X_train_cpy['T'] = (X_train_cpy.NWP1_T + X_train_cpy.NWP3_T)/2\n",
    "    X_train_cpy['CLCT'] = (X_train_cpy.NWP4_CLCT)\n",
    "    X_train_cpy['inv_T'] = 1/(X_train_cpy['T'])\n",
    "    \n",
    "    X_test_cpy['U'] = (X_test_cpy.NWP1_U + X_test_cpy.NWP2_U + X_test_cpy.NWP3_U)/3\n",
    "    X_test_cpy['V'] = (X_test_cpy.NWP1_V + X_test_cpy.NWP2_V + X_test_cpy.NWP3_V)/3\n",
    "    X_test_cpy['T'] = (X_test_cpy.NWP1_T + X_test_cpy.NWP3_T)/2\n",
    "    X_test_cpy['CLCT'] = (X_test_cpy.NWP4_CLCT)\n",
    "    X_test_cpy['inv_T'] = 1/(X_test_cpy['T'])\n",
    "    \n",
    "    X_train_cpy = X_train_cpy[['Time','U','V','T','CLCT','inv_T']]\n",
    "    X_test_cpy = X_test_cpy[['Time','U','V','T','CLCT', 'inv_T']]\n",
    "\n",
    "    \n",
    "    ## Limpieza de datos: imputar valores perdidos en X_\n",
    "    #only_na = X_test_cpy[~X_test_cpy.ID.isin(X_test_cpy.dropna().ID)]\n",
    "    #X_test_cpy.dropna(inplace=True)\n",
    "    #Y_train_cpy.drop(labels=only_na.index, axis=0, inplace=True)\n",
    "    #X_train_cpy.drop(labels=only_na.index, axis=0, inplace=True)\n",
    "\n",
    "    # Pre-procesado de los datos\n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [('drop_columns', 'drop', \n",
    "                                                     ['Time','U','V','T'])]\n",
    "    )\n",
    "\n",
    "    # definir pipeline\n",
    "    prepare_data_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', dtr.DerivedAttributesAdder(add_time_feat=False, add_cyclic_feat=False)), \n",
    "        ('pre_processing', pre_process),\n",
    "        ('std_scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # aplicar pipeline\n",
    "    X_train_pped = prepare_data_pipeline.fit_transform(X_train_cpy)\n",
    "\n",
    "\n",
    "    # creamos nuestro scorer basado en el CAPE\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "\n",
    "    # Modelización: Regresión polinomial utilizando CV con regularización tipo Ride\n",
    "    poly_features = PolynomialFeatures(degree=3, include_bias=False)\n",
    "    X_train_poly = poly_features.fit_transform(X_train_pped)\n",
    "    \n",
    "    param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "    lasso_reg = lm.Lasso()\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        lasso_reg, \n",
    "        param_grid, \n",
    "        cv=7,\n",
    "        scoring=cape_scorer,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train_poly, Y_train_cpy)\n",
    "    final_model = grid_search.best_estimator_\n",
    "    \n",
    "    # guardamos modelo\n",
    "    models.append(joblib.dump(final_model, WF + '_lasso'))\n",
    "\n",
    "    # evaluación sobre el conjunto de test\n",
    "    X_test_pped = prepare_data_pipeline.transform(X_test_cpy)\n",
    "    X_test_poly = poly_features.fit_transform(X_test_pped)\n",
    "    predictions = final_model.predict(X_test_poly)\n",
    "    \n",
    "    # generamos matriz de predicciones (ID,Production)\n",
    "    pred_matrix = np.stack((np.array(ID_test).astype(int), predictions), axis=-1)\n",
    "    df_pred = pd.DataFrame(data=pred_matrix, columns=['ID','Production'])\n",
    "\n",
    "    # añadimos las predicciones al dataframe final que contendrá las de todas las WF\n",
    "    submission_df = submission_df.append(df_pred, ignore_index=True) \n",
    "    print('CAPE for {} is {}'.format(WF, -grid_search.best_score_))\n",
    "    print('Predictions for {} has been added to submission_df'.format(WF))\n",
    "\n",
    "# generamos fichero csv para el submission\n",
    "submission_df.to_csv(\"../../TFM/models/submission_lasso.csv\", index=False, sep=\",\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 8: Regresión robusta con RANSAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Automatización de pre-procesado, modelización, validación y prediccion\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.functions import metric\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model as lm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = X_train['WF'].unique()\n",
    "\n",
    "# lista para guardar los modelos, uno por WF\n",
    "models = []\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "    X_train_cpy = X_train_cpy[['Time','NWP1_00h_D-1_U','NWP1_00h_D-1_V','NWP1_00h_D-1_T']]\n",
    "    X_train_cpy.rename(\n",
    "        columns={\"Time\": \"time\", \"NWP1_00h_D-1_U\": \"U\", \"NWP1_00h_D-1_V\": \"V\", \"NWP1_00h_D-1_T\":\"T\"}, \n",
    "        inplace=True)\n",
    "\n",
    "    X_test_cpy = X_test_cpy[['Time','NWP1_00h_D-1_U','NWP1_00h_D-1_V','NWP1_00h_D-1_T']]\n",
    "    X_test_cpy.rename(\n",
    "        columns={\"Time\": \"time\", \"NWP1_00h_D-1_U\": \"U\", \"NWP1_00h_D-1_V\": \"V\", \"NWP1_00h_D-1_T\":\"T\"}, \n",
    "        inplace=True)\n",
    "\n",
    "    # Limpieza de datos: eliminar valores perdidos\n",
    "    only_na = X_train_cpy[~X_train_cpy.index.isin(X_train_cpy.dropna().index)]\n",
    "    X_train_cpy.dropna(inplace=True)\n",
    "    Y_train_cpy.drop(labels=only_na.index, axis=0, inplace=True)\n",
    "\n",
    "    X_test_cpy.dropna(inplace=True)\n",
    "\n",
    "    # Pre-procesado de los datos\n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [('drop_columns', 'drop', ['time','U','V','T','w_dir'])]\n",
    "    )\n",
    "\n",
    "    # definir pipeline\n",
    "    prepare_data_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', dtr.DerivedAttributesAdder(add_time_feat=False)), \n",
    "        ('pre_processing', pre_process),\n",
    "        ('std_scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # aplicar pipeline\n",
    "    X_train_pped = prepare_data_pipeline.fit_transform(X_train_cpy)\n",
    "\n",
    "    # creamos nuestro scorer basado en el CAPE\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "\n",
    "    # Modelización: Regresión Robusta utilizando RANSAC\n",
    "    poly_features = PolynomialFeatures(degree=3, include_bias=False)\n",
    "    X_train_poly = poly_features.fit_transform(X_train_pped)\n",
    "    \n",
    "    ransac = RANSACRegressor(LinearRegression(min_sa), loss='absolute_loss')\n",
    "    param_grid = [\n",
    "        {\n",
    "            'max_trials': [100, 1000, 10000],\n",
    "            'min_samples': [10, 30, 50],\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    grid_search_ransac = GridSearchCV(\n",
    "        ransac, \n",
    "        param_grid, \n",
    "        cv=5,\n",
    "        scoring=cape_scorer\n",
    "    )\n",
    "\n",
    "    grid_search_ransac.fit(X_train_poly, Y_train_cpy)\n",
    "    final_model = grid_search_ransac.best_estimator_\n",
    "    \n",
    "    # guardamos modelo\n",
    "    models.append(joblib.dump(final_model, WF + '_ransac'))\n",
    "\n",
    "    # evaluación sobre el conjunto de test\n",
    "    X_test_pped = prepare_data_pipeline.transform(X_test_cpy)\n",
    "    X_test_poly = poly_features.fit_transform(X_test_pped)\n",
    "    predictions = final_model.predict(X_test_poly)\n",
    "    \n",
    "    # generamos matriz de predicciones (ID,Production)\n",
    "    pred_matrix = np.stack((np.array(ID_test).astype(int), predictions), axis=-1)\n",
    "    df_pred = pd.DataFrame(data=pred_matrix, columns=['ID','Production'])\n",
    "\n",
    "    # añadimos las predicciones al dataframe final que contendrá las de todas las WF\n",
    "    submission_df = submission_df.append(df_pred, ignore_index=True) \n",
    "    print('Predictions for {} has been added to submission_df'.format(WF))\n",
    "    print('CAPE for {} is {}'.format(WF, -grid_search_ransac.best_score_))\n",
    "\n",
    "# generamos fichero csv para el submission\n",
    "submission_df.to_csv(\"./Data/submission_ransac.csv\", index=False, sep=\",\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo 10: MARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameter values:  {'mars__allow_linear': True, 'mars__max_degree': 3, 'mars__penalty': 5.0, 'univariate_sel__k': 2}\n",
      "CAPE: 24.522433178494186\n",
      "Selected Features:  ['wspeed', 'wspeed_invT']\n",
      "Predictions for WF2 has been added to submission_df\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "# Automatización de pre-procesado, modelización, validación y prediccion\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.functions import metric\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model as lm\n",
    "from pyearth import Earth\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "np.warnings.filterwarnings('ignore')\n",
    "import hdbscan\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "\n",
    "# Lista de WFs\n",
    "WF_lst = ['WF2']\n",
    "\n",
    "# lista para guardar los modelos, uno por WF\n",
    "models = []\n",
    "scores_list = []\n",
    "\n",
    "# Data frame para guardar las predicciones (ID, Production)\n",
    "submission_df = pd.DataFrame([], columns=['ID','Production']) \n",
    "\n",
    "for WF in WF_lst:\n",
    "    # Creamos copia de los datos para no perder su formato inicial\n",
    "    X_train_cpy = X_train.copy()\n",
    "    Y_train_cpy = Y_train.copy()\n",
    "    X_test_cpy = X_test.copy()\n",
    "\n",
    "    # Selección de filas por WF\n",
    "    X_train_cpy = X_train_cpy[X_train_cpy['WF'] == WF]\n",
    "    X_test_cpy = X_test_cpy[X_test_cpy['WF'] == WF]\n",
    "\n",
    "    # Identificador de observaciones\n",
    "    ID_train = X_train_cpy['ID']\n",
    "    ID_test = X_test_cpy['ID']\n",
    "\n",
    "    # Seleccion de las filas de Y_train\n",
    "    Y_train_cpy = Y_train_cpy.loc[ID_train.values - 1]\n",
    "\n",
    "    # Pre-procesado de los datos en bruto\n",
    "\n",
    "    # We'll add new columns NWPX_<met_var> without missing values\n",
    "    # new cols: NWP1_U, NWP1_V, NWP1_T, NWP2_U, NWP2_V, NWP3_U, NWP3_V, NWP3_T, NWP4_U, NWP4_V, NWP4_CLCT\n",
    "    \n",
    "    new_cols = ['NWP1_U','NWP1_V','NWP1_T','NWP2_U',\n",
    "                'NWP2_V','NWP3_U','NWP3_V','NWP3_T',\n",
    "                'NWP4_U','NWP4_V','NWP4_CLCT']\n",
    "\n",
    "    def add_new_cols(new_cols, df):\n",
    "        for col in new_cols:\n",
    "            df[col] = np.nan    \n",
    "            \n",
    "    add_new_cols(new_cols, X_train_cpy)\n",
    "    add_new_cols(new_cols, X_test_cpy)\n",
    "    \n",
    "    cols_train = X_train.columns[3:]\n",
    "    cols_test = X_test.columns[3:-9]\n",
    "    X_train_cpy = input_missing_values(X_train_cpy, cols_train)\n",
    "    X_test_cpy = input_missing_values(X_test_cpy, cols_train)\n",
    "    \n",
    "    col_list = ['NWP2_U','NWP2_V','NWP3_U','NWP3_V','NWP3_T','NWP4_CLCT']\n",
    "    X_train_cpy.index = X_train_cpy['Time']\n",
    "    \n",
    "    del X_train_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_train_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=100,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "        \n",
    "    X_train_cpy.reset_index(inplace=True)\n",
    "    \n",
    "    X_test_cpy.index = X_test_cpy['Time']\n",
    "    \n",
    "    del X_test_cpy['Time']\n",
    "        \n",
    "    for var in col_list:\n",
    "        X_test_cpy[var].interpolate(\n",
    "            method='time', \n",
    "            inplace=True,\n",
    "            limit=100,\n",
    "            limit_direction='both'\n",
    "        )\n",
    "    \n",
    "      \n",
    "    X_test_cpy.reset_index(inplace=True) \n",
    "\n",
    "    X_train_cpy['U'] = (X_train_cpy.NWP1_U + X_train_cpy.NWP2_U + X_train_cpy.NWP3_U)/3\n",
    "    X_train_cpy['V'] = (X_train_cpy.NWP1_V + X_train_cpy.NWP2_V + X_train_cpy.NWP3_V )/3\n",
    "    X_train_cpy['T'] = (X_train_cpy.NWP1_T + X_train_cpy.NWP3_T)/2\n",
    "    X_train_cpy['CLCT'] = X_train_cpy.NWP4_CLCT\n",
    "    \n",
    "    X_test_cpy['U'] = (X_test_cpy.NWP1_U + X_test_cpy.NWP2_U + X_test_cpy.NWP3_U)/3\n",
    "    X_test_cpy['V'] = (X_test_cpy.NWP1_V + X_test_cpy.NWP2_V + X_test_cpy.NWP3_V)/3\n",
    "    X_test_cpy['T'] = (X_test_cpy.NWP1_T + X_test_cpy.NWP3_T)/2\n",
    "    X_test_cpy['CLCT'] = X_test_cpy.NWP4_CLCT\n",
    " \n",
    "    X_train_cpy = X_train_cpy[['ID','Time','U','V','T','CLCT']]\n",
    "    X_test_cpy = X_test_cpy[['ID','Time','U','V','T','CLCT']]\n",
    "\n",
    "    ####### Limpieza de outliers ########\n",
    "    X_train_cpy.loc[X_train_cpy['CLCT'] < 0, 'CLCT'] = 0.0\n",
    "    X_test_cpy.loc[X_test_cpy['CLCT'] < 0, 'CLCT'] = 0.0\n",
    "    \n",
    "    # añadir columna Production\n",
    "    X_train_cpy['Production'] = list(Y_train_cpy['Production'])\n",
    "    \n",
    "    # calcular módulo velocidad del viento\n",
    "    X_train_cpy['vel'] = X_train_cpy.apply(get_wind_velmod, axis=1)\n",
    "    \n",
    "    X_ = X_train_cpy[['vel','Production']]\n",
    "    \n",
    "    # parámetros por granja\n",
    "    if WF == 'WF1':\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.1\n",
    "        frac_std = 0.5\n",
    "        threshold_type = 'scalar'\n",
    "        bottom_max = 6.\n",
    "    elif WF == \"WF2\":\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.1\n",
    "        frac_std = 0.7\n",
    "        threshold_type = 'scalar'\n",
    "        bottom_max = 5.3\n",
    "    elif WF == \"WF3\":\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.1\n",
    "        frac_std = 0.55\n",
    "        threshold_type = 'scalar'\n",
    "        bottom_max = 5.\n",
    "    elif WF == \"WF4\":\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.1\n",
    "        frac_std = 0.95\n",
    "        threshold_type = 'scalar'\n",
    "        bottom_max = 5.3\n",
    "    elif WF == \"WF5\":\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.1\n",
    "        frac_std = 0.55\n",
    "        threshold_type = 'scalar'\n",
    "        bottom_max = 5.\n",
    "    else:\n",
    "        top_frac_max = 0.97\n",
    "        sparse_bin_width = 0.05\n",
    "        frac_std = 0.85\n",
    "        threshold_type = 'scalar'\n",
    "        bottom_max = 5.\n",
    "        \n",
    "    # top-curve stacked outliers\n",
    "    top_stacked = filters.window_range_flag(\n",
    "        X_.Production, top_frac_max * X_.Production.max(), X_.Production.max(), X_.vel, 12.5, 2000.\n",
    "    )\n",
    "    \n",
    "    # sparse outliers\n",
    "    max_bin = 0.97*X_['Production'].max()\n",
    "    sparse_outliers = filters.bin_filter(\n",
    "        X_.Production, X_.vel, sparse_bin_width, frac_std * X_.Production.std(), 'median', 0.1, max_bin, threshold_type, 'all'\n",
    "    )\n",
    "    \n",
    "    # bottom-curve stacked outliers\n",
    "    bottom_stacked = filters.window_range_flag(X_.vel, bottom_max, 40, X_.Production, 0.05, 2000.)\n",
    "    \n",
    "    # deleting outliers\n",
    "    X_.vel = X_.vel[(~top_stacked) & (~sparse_outliers) & (~bottom_stacked)]\n",
    "    X_.Production = X_.Production[(~top_stacked) & (~sparse_outliers) & (~bottom_stacked)]\n",
    "    plot_flagged_pc(X_.vel, X_.Production, np.repeat('True', len(X_.vel)), 0.7)\n",
    "    \n",
    "    # seleccionar filas correspondientes a los no-outliers\n",
    "    X_train_cpy = X_train_cpy.loc[X_train_cpy['vel'].isin(X_.vel)]\n",
    "    Y_train_cpy = Y_train_cpy.loc[Y_train_cpy['ID'].isin(X_train_cpy['ID'])]\n",
    "    Y_train_cpy = Y_train_cpy['Production']\n",
    "\n",
    "    \n",
    "    # Eliminamos las columnas 'vel' y 'Production'\n",
    "    del X_train_cpy['vel']\n",
    "    del X_train_cpy['Production']\n",
    "    ###################################\n",
    "\n",
    "    ## Feature engineering pipeline #####\n",
    "    feat_adder = dtr.NewFeaturesAdder(add_time_feat=True, add_cycl_feat=True, add_inv_T=True,add_interactions=True)\n",
    "    \n",
    "    drop_lst = []\n",
    "    if feat_adder.get_params().get('add_cycl_feat'):\n",
    "        if feat_adder.get_params().get('add_inv_T'):\n",
    "            drop_lst = [\"ID\", \"Time\", \"U\", \"V\", \"wdir\", \"hour\", \"month\", \"T\"]\n",
    "        else:\n",
    "            drop_lst = [\"ID\", \"Time\", \"U\", \"V\", \"wdir\", \"hour\", \"month\"]\n",
    "    else:\n",
    "        if feat_adder.get_params().get('add_inv_T'):\n",
    "            drop_lst = [\"ID\", \"Time\", \"U\", \"V\", \"T\"]\n",
    "        else:\n",
    "            drop_lst = [\"ID\", \"Time\", \"U\", \"V\"]\n",
    "    \n",
    "    pre_process = ColumnTransformer(remainder = 'passthrough',\n",
    "                                    transformers = [(\n",
    "                                        'drop_columns', 'drop', drop_lst)\n",
    "                                    ])\n",
    "    \n",
    "    # transforming target because of its skweed distribution.\n",
    "    tt = Pipeline(steps=[\n",
    "        ('powertransformer', PowerTransformer(method='yeo-johnson', standardize=True)), \n",
    "    ])\n",
    "    \n",
    "    feat_eng_pipeline = Pipeline(steps=[\n",
    "        ('attr_adder', feat_adder), \n",
    "        ('pre_processing', pre_process),\n",
    "        ('powertransformer', PowerTransformer(method='yeo-johnson', standardize=True)),   \n",
    "    ])\n",
    "    \n",
    "    \n",
    "    X_train_pped = feat_eng_pipeline.fit_transform(X_train_cpy)\n",
    "    X_test_pped = feat_eng_pipeline.transform(X_test_cpy)\n",
    "    \n",
    "    #### \n",
    "    # Feature selection\n",
    "    feature_names = X_train_cpy.drop(drop_lst, axis=1).columns\n",
    "    selec_k_best = SelectKBest(mutual_info_regression, k=1)        \n",
    "    \n",
    "    # make scorers\n",
    "    cape_scorer = make_scorer(metric.get_cape, greater_is_better=False)\n",
    "    \n",
    "    pipeline = Pipeline([(\"univariate_sel\", selec_k_best), (\"mars\", Earth(feature_importance_type='gcv'))] )\n",
    "    \n",
    "    ## Modeling: MARS using py-earth ######\n",
    "    param_grid = {'mars__max_degree': [3,4,5], \n",
    "                  'mars__allow_linear': [False, True], \n",
    "                  'mars__penalty': [0.,1.,2.,3.,4.,5.,6.],\n",
    "                  'univariate_sel__k': [1,2],\n",
    "                  }\n",
    "    \n",
    "    # Cross validation and hyper-parameter tunning with grid seach\n",
    "    n_splits = 7\n",
    "    tscv = TimeSeriesSplit(n_splits)\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline, \n",
    "        param_grid, \n",
    "        cv=tscv,\n",
    "        scoring=cape_scorer,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train_pped, Y_train_cpy)\n",
    "    \n",
    "    # Feature selection with the best k value obtanined in Grid Search\n",
    "    selec_k_best = SelectKBest(mutual_info_regression, k=grid_search.best_params_['univariate_sel__k'])\n",
    "    selec_k_best.fit(X_train_pped, Y_train_cpy.to_numpy().reshape(-1))\n",
    "    X_train_pped = selec_k_best.transform(X_train_pped)\n",
    "    X_test_pped = selec_k_best.transform(X_test_pped)\n",
    "    \n",
    "    mask =selec_k_best.get_support() #list of booleans\n",
    "    selected_feat = [] \n",
    "\n",
    "    for bool, feature in zip(mask, feature_names):\n",
    "        if bool:\n",
    "            selected_feat.append(feature)  \n",
    "\n",
    "        # Re-training without CV, using the best param#eters obtained by CV\n",
    "        mars = Earth(max_degree=grid_search.best_params_['mars__max_degree'],\n",
    "                 allow_linear=grid_search.best_params_['mars__allow_linear'],\n",
    "                 penalty=grid_search.best_params_['mars__penalty']\n",
    "                )\n",
    "\n",
    "    mars_ttreg = TransformedTargetRegressor(regressor=mars, \n",
    "                                   transformer=tt, \n",
    "                                   check_inverse=False)\n",
    "    mars_ttreg.fit(X_train_pped, Y_train_cpy)\n",
    "    \n",
    "\n",
    "    # Re-training without CV, using the best param#eters obtained by CV\n",
    "    mars = Earth(max_degree=grid_search.best_params_['mars__max_degree'],\n",
    "             allow_linear=grid_search.best_params_['mars__allow_linear'],\n",
    "            penalty=grid_search.best_params_['mars__penalty']\n",
    "            )\n",
    "\n",
    "    mars_ttreg = TransformedTargetRegressor(regressor=mars, \n",
    "                                   transformer=tt, \n",
    "                                   check_inverse=False)\n",
    "\n",
    "    mars_ttreg = TransformedTargetRegressor(regressor=mars, \n",
    "                                   transformer=tt, \n",
    "                                   check_inverse=False)\n",
    "    mars_ttreg.fit(X_train_pped, Y_train_cpy)\n",
    "    \n",
    "    # predicting on test data\n",
    "    predictions = mars_ttreg.predict(X_test_pped)\n",
    "\n",
    "    # build prediction matrix (ID,Production)\n",
    "    pred_matrix = np.stack((np.array(ID_test).astype(int), predictions), axis=-1)\n",
    "    df_pred = pd.DataFrame(data=pred_matrix.reshape(-1,2), columns=['ID','Production'])\n",
    "    \n",
    "    # corregimos valores negativos\n",
    "    df_pred.loc[df_pred['Production'] < 0, 'Production'] = 0.0\n",
    "\n",
    "    # añadimos las predicciones al dataframe final que contendrá las de todas las WF\n",
    "    submission_df = submission_df.append(df_pred, ignore_index=True)\n",
    "    print('Best hyperparameter values: ',grid_search.best_params_)\n",
    "    print('CAPE:', -grid_search.best_score_)\n",
    "    print('Selected Features: ', selected_feat)\n",
    "    print('Predictions for {} has been added to submission_df'.format(WF))\n",
    "    print('---------')\n",
    "    \n",
    "# generamos fichero csv para el submission\n",
    "submission_df.to_csv(\"../../../../TFM/models/mars_all_feat.csv\", index=False, sep=\",\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-04 18:01:30,913 - kedro.io.data_catalog - INFO - Loading data from `X_train_raw` (CSVDataSet)...\n",
      "2020-10-04 18:01:31,291 - kedro.io.data_catalog - INFO - Loading data from `X_test_raw` (CSVDataSet)...\n",
      "2020-10-04 18:01:31,570 - kedro.io.data_catalog - INFO - Loading data from `y_train_raw` (CSVDataSet)...\n"
     ]
    }
   ],
   "source": [
    "X_train_raw = ctx.catalog.load(\"X_train_raw\")\n",
    "X_test_raw = ctx.catalog.load(\"X_test_raw\")\n",
    "y_train_raw = ctx.catalog.load(\"y_train_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: ../../data/06_models/WF1/ANN.h5/{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-131-cf073701ae4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msource_folder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"../../data/06_models/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../../data/06_models/WF1/ANN.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\quark\\.conda\\envs\\tfm-env\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m       \u001b[0mloader_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\quark\\.conda\\envs\\tfm-env\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[1;34m(export_dir)\u001b[0m\n\u001b[0;32m    111\u001b[0m                   (export_dir,\n\u001b[0;32m    112\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m                    constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: SavedModel file does not exist at: ../../data/06_models/WF1/ANN.h5/{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "source_folder = \"../../data/06_models/\"\n",
    "model = tf.keras.models.load_model(\"../../data/06_models/WF1/ANN.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/06_models/'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"../../data/06_models/WF1/ANN.h5\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WindPowerForecasting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
