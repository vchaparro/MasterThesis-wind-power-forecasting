{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index\n",
    "\n",
    "* [Data Preparation](#data_prepare)\n",
    "    - [Missing Values](#missing_values)\n",
    "    - [Feature Engineering](#feat_engineering)\n",
    "        - [Wind Velocity related features](#wind_feat)\n",
    "        - [Time related features](#time_feat)\n",
    "        - [EDA of new features](#new_feat_eda)\n",
    "        - [Feature Importance](#feat_importance)\n",
    "        - [Feature transformations](#feat_transformation)\n",
    "    - [Target transformation](#target_transfomation)\n",
    "    - [Outliers and abnormal data](#outliers)\n",
    "    - [Best Numerical Weather Predictor](#bestnwp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import gc\n",
    "import missingno as msno\n",
    "import pandas_profiling\n",
    "import statsmodels as sm\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import random\n",
    "\n",
    "from src.functions import data_import as dimp\n",
    "from src.functions import data_exploration as dexp\n",
    "from src.functions import data_transformation as dtr\n",
    "from src.functions import utils as utl\n",
    "\n",
    "#visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly as pty\n",
    "import re\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import cufflinks as cf\n",
    "cf.set_config_file(offline=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "<a id=\"data_prepare\"></a>\n",
    "\n",
    "We'll create a pipeline with scikit learn to perform the data preparation including the transformations identified in EDA, recall here:\n",
    "* Input missing values\n",
    "* Create new features:\n",
    "    - Wind related features\n",
    "        - Wind velocity module.\n",
    "        - Wind direction.\n",
    "        - Wind velocity escaled to heigth of the turbine. We can calculate it by using the *Hellmann power equation*:\n",
    "    \n",
    "            $$u(z) = u(z_0)\\left(\\frac{z}{z_0}\\right)^{\\alpha},$$ \n",
    "      \n",
    "          with $\\alpha = 1/7$, $z_0 = 100$ or $10$ meters, depending on the NWP data, and $z = 50$ m, the height of the \n",
    "          turbines.\n",
    "          \n",
    "    - Date time future enconding to capture seasonality:\n",
    "        - Diurnal variations of average wind speeds (hour feature).\n",
    "        - Monthly variations of averge wind speeds  (month feature).\n",
    "\n",
    "\n",
    "* Stardard Scaling of variables\n",
    "* Temperature `T` in $^\\text{o}$C instead of Kelvin\n",
    "* Outliers and abnormal data:\n",
    "    - Abnormal data: `CLCT` negative values.\n",
    "    - Outliers: using power curve as a reference to deal with typical outlier types in this context:\n",
    "        - Data points in low wind speed period with high power generation.\n",
    "        - Data points with negtative value wind speed.\n",
    "        - Data points with negative valu power generation.\n",
    "        - Data points with low power generation at high wind speed period.\n",
    "* Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw = catalog.load(\"X_train_raw\")\n",
    "X_test_raw = catalog.load(\"X_test_raw\")\n",
    "y_train_raw = catalog.load(\"y_train_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows only for WF1\n",
    "X_train = X_train[X_train.WF == 'WF1']\n",
    "X_test = X_test_2[X_test_2.WF == 'WF1']\n",
    "y_train = Y_train_2[Y_train_2.ID.isin(X_train.ID)]\n",
    "y_test = Y_test_2[Y_test_2.ID.isin(X_test.ID)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep IDs separately\n",
    "ID_X_train = X_train['ID']\n",
    "ID_X_test = X_test['ID']\n",
    "ID_y_train = y_train['ID']\n",
    "ID_y_test = y_test['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.columns[3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values\n",
    "<a id=\"missing_values\"></a>\n",
    "\n",
    "**Assumption**: Meteorological forecasts accuracy and reliability are the highest for Forecast Day `D` and Run `18h` (the shorter the gap time between NWP Run and the time forecasted, the higher the realiability).\n",
    "\n",
    "\n",
    "For `X_train`:\n",
    "We'll use whenever it's possible, data from forecast day `D` and run `18h` for every `NWP` as they're the most recent data available. When there're missing values, we'll use the corresponding data from previous days to fill in the gap, sequencely.\n",
    "\n",
    "Moreover, `NWP2` and `NWP3` provides data every 3 hours, instead of every 1 hour, as `NWP1` and `NWP4` do. \n",
    "* `NWP1`: day 13/07/2018 is fully missing. We'll use `D-1` forecasts to input these values.\n",
    "* `NWP4`: day 15/05/2018 is fully missing. We'll use `D-1` forecasts to input these values.\n",
    "* `NWP2` and `NWP3`: we will interpolate these values, missing three hours in between every data.\n",
    "\n",
    "For `X_test`:\n",
    "The rules of the challenge point out that we can only use data available on day `D` at 09:00 h (practically that means forecast from day `D-1` and Run 00h). We have then the following missing values to input:\n",
    "\n",
    "* `NWP1` and `NWP4`: no missing values for the most recent data (forecast day D-1, Run 00h).\n",
    "* `NWP2` and `NWP3`: we will interpolate these values, missing three hours in between every data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll add new columns NWPX_<met_var> without missing values\n",
    "new_cols = ['NWP1_U', 'NWP1_V', 'NWP1_T', 'NWP2_U', \n",
    "            'NWP2_V', 'NWP3_U', 'NWP3_V', 'NWP3_T',\n",
    "            'NWP4_U', 'NWP4_V', 'NWP4_CLCT']      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input missing values in X_train\n",
    "X_train_cpy = X_train.copy()\n",
    "cols_train = X_train_cpy.columns[3:] \n",
    "new_cols = ['NWP1_U','NWP1_V','NWP1_T',\n",
    "           'NWP2_U','NWP2_V',\n",
    "           'NWP3_U','NWP3_V','NWP3_T',\n",
    "           'NWP4_U','NWP4_V','NWP4_CLCT']\n",
    "\n",
    "dtr.add_new_cols(new_cols, X_train_cpy)\n",
    "dtr.input_missing_values(X_train_cpy, cols_train)\n",
    "\n",
    "col_list = ['NWP2_U','NWP2_V','NWP3_U','NWP3_V','NWP3_T']\n",
    "dtr.interpolate_missing_values(X_train_cpy, col_list, 'Time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the consistency of the data after the missing values inputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[['NWP4_00h_D_U','NWP4_12h_D_U']].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cpy[new_cols].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All seems OK for `X_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# input missing values in X_test\n",
    "X_test_cpy = X_test.copy()\n",
    "cols_test = X_test_cpy.columns[3:-9] \n",
    "\n",
    "dtr.add_new_cols(new_cols, X_test_cpy)\n",
    "dtr.input_missing_values(X_test_cpy, cols_test)\n",
    "\n",
    "col_list = ['NWP2_U','NWP2_V','NWP3_U','NWP3_V','NWP3_T']\n",
    "dtr.interpolate_missing_values(X_test_cpy, col_list, 'Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_cpy[new_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All seems OK also for `X_test`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "<a id=\"feat_engineering\"></a>\n",
    "\n",
    "### Wind Velocity related features\n",
    "<a id=\"wind_feat\"></a>\n",
    "\n",
    "Adding the following wind velocity derived features for each NWP:\n",
    "* `NWP<1,2,3,4>_wvel`\n",
    "* `NWP<1,2,3,4>_wvel`\n",
    "* `NWP<1,2,3,4>_wshear`\n",
    "* `NWP<1,2,3,4>_wdir`\n",
    "* `NWP<1,2,3,4>_wdir_sin`\n",
    "* `NWP<1,2,3,4>_wdir_cos`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add wind velocity related features in X_train_cpy\n",
    "regex = r'NWP(?P<NWP>\\d{1})_(?P<met_var>[UV])'\n",
    "dtr.add_wind_vars(X_train_cpy, regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add wind velocity related features in X_test_cpy\n",
    "dtr.add_wind_vars(X_test_cpy, regex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time related features \n",
    "<a id=\"time_feat\"></a>\n",
    "\n",
    "Adding the following date time velocity derived features:\n",
    "* `hour`\n",
    "* `month`\n",
    "* `hour_sin`\n",
    "* `hour_cos`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add time related features in X_train_cpy\n",
    "dtr.add_time_vars(X_train_cpy, 'Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add time related features in X_test_cpy\n",
    "dtr.add_time_vars(X_test_cpy, 'Time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature conversion to $^{\\text{o}}$C\n",
    "<a id=\"temp_conv\"></a>\n",
    "\n",
    "Just for an easier interpretation of temperature data, we'll convert it to Celsius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cpy[['NWP1_T','NWP3_T']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA of new features\n",
    "<a id=\"new_feat_eda\"></a>\n",
    "\n",
    "Let's come back to EDA now to visualize the distributions, dependencies and behaviour of the new added features.\n",
    "\n",
    "#### Wind velocity distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda = X_train_cpy.copy()\n",
    "df_eda['Production'] = y_train['Production'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Group data together\n",
    "hist_data = [df_eda['NWP1_wvel'],df_eda['NWP2_wvel'],df_eda['NWP3_wvel'],df_eda['NWP4_wvel']]\n",
    "group_labels = ['NWP1 wind vel', 'NWP2 wind vel', 'NWP3 wind vel', 'NWP4 wind vel']\n",
    "\n",
    "# Create distplot with custom bin_size\n",
    "fig = ff.create_distplot(hist_data, group_labels, bin_size=.2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series visualization\n",
    "df_eda['NWP3_T_cels'] = df_eda['NWP3_T'] - 273\n",
    "df_eda[\n",
    "    ['Time','NWP1_wvel','NWP1_wdir', 'NWP3_T_cels']].set_index('Time').iplot(\n",
    "    kind='scatter', \n",
    "    filename='cufflinks/cf-simple-line'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda.loc[df_eda['NWP3_wvel'] == max(df_eda['NWP3_wvel'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda.loc[df_eda['NWP3_wvel'] == min(df_eda['NWP3_wvel'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependency between wind power and wind velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=2, \n",
    "                    cols=2, \n",
    "                    start_cell=\"top-left\",\n",
    "                    x_title='wind velocity [m/s]',\n",
    "                    y_title='wind power [MWh]')\n",
    "\n",
    "fig.add_trace(go.Scatter(x=df_eda['NWP1_wvel'], y=df_eda['Production'], mode=\"markers\", marker=dict(size=3), name='NWP1'),\n",
    "              row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=df_eda['NWP2_wvel'], y=df_eda['Production'], mode=\"markers\", marker=dict(size=3), name='NWP2'),\n",
    "              row=1, col=2)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=df_eda['NWP3_wvel'], y=df_eda['Production'], mode=\"markers\", marker=dict(size=3), name='NWP3'),\n",
    "              row=2, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=df_eda['NWP4_wvel'], y=df_eda['Production'], mode=\"markers\", marker=dict(size=3),  name='NWP4'),\n",
    "              row=2, col=2)\n",
    "\n",
    "fig.update_layout(height=800, width=800)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wind direction distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import windrose\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_subplot(111, projection=\"windrose\")\n",
    "ax.bar(df_eda['NWP1_wdir'], df_eda['NWP1_wvel'], normed=True, opening=0.8, edgecolor='white')\n",
    "# ax.set_legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, \n",
    "                    cols=1, \n",
    "                    start_cell=\"top-left\",\n",
    "                    x_title='wind direction',\n",
    "                    y_title='wind power production [MWh]')\n",
    "\n",
    "fig.add_trace(go.Scatter(x=df_eda['NWP1_wdir'], y=df_eda['Production'], mode=\"markers\", marker=dict(size=3), name='NWP1'),\n",
    "              row=1, col=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wind velocity and direction versus power production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=df_eda['NWP1_wvel'],\n",
    "    y=df_eda['NWP1_wdir'],\n",
    "    z=df_eda['Production'],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=3,\n",
    "        color=df_eda['Production'],            \n",
    "        colorscale='Viridis', \n",
    "        opacity=0.8\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(autosize=False,\n",
    "                  width=500, height=500,\n",
    "                  margin=dict(l=65, r=50, b=65, t=90))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of dependencies between wind power and meteorological variables (T and CLCT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig = make_subplots(rows=1, \n",
    "                    cols=2, \n",
    "                    start_cell=\"top-left\",\n",
    "                    x_title='air temperature [K]',\n",
    "                    y_title='power production [MWh]')\n",
    "\n",
    "fig.add_trace(go.Scatter(x=df_eda['NWP1_T'], y=df_eda['Production'], mode=\"markers\", marker=dict(size=3), name='NWP1'),\n",
    "              row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=df_eda['NWP3_T'], y=df_eda['Production'], mode=\"markers\", marker=dict(size=3), name='NWP3'),\n",
    "              row=1, col=2)\n",
    "\n",
    "fig.update_layout(height=500, width=800)\n",
    "\n",
    "fig.show()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig = make_subplots(rows=1, \n",
    "                    cols=1, \n",
    "                    start_cell=\"top-left\",\n",
    "                    x_title='taotal cloud cover [%]',\n",
    "                    y_title='power production[MWh]')\n",
    "\n",
    "fig.add_trace(go.Scatter(x=df_eda['NWP4_CLCT'], \n",
    "                         y=df_eda['Production'], \n",
    "                         mode=\"markers\", marker=dict(size=3), name='NWP4'),\n",
    "              row=1, col=1)\n",
    "\n",
    "\n",
    "fig.update_layout(height=500, width=800)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at correlations\n",
    "df_eda[['Production','NWP1_U', 'NWP1_V','NWP1_wvel','NWP1_wshear',\n",
    "        'NWP1_T','NWP1_wdir','NWP1_wdir_sin','NWP1_wdir_cos','month',\n",
    "       'month_sin','month_cos', 'hour','hour_sin','hour_cos']].corr().iplot(\n",
    "    kind='heatmap', \n",
    "    colorscale='spectral', \n",
    "    filename='cufflinks/simple-heatmap'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance\n",
    "<a id=\"feat_importance\"></a>\n",
    "\n",
    "Random Forest can be used to get the importance of the features in a dataset. We will drop all original features that are redudant with dervied ones (high correlated), for example, wind velocity, U and V if wind shear is included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature importance for NWP1\n",
    "\n",
    "from sklearn.ensemble import  RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# data\n",
    "X = X_train_cpy[['NWP1_T',\n",
    "                'NWP1_wshear','NWP1_wdir_cos','NWP1_wdir_sin',\n",
    "                'hour_cos','hour_sin',\n",
    "                'month_sin','month_cos']]\n",
    "Y = y_train['Production']\n",
    "\n",
    "# split into train and test sets\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size=0.20, random_state=0)\n",
    "\n",
    "# instanciate object RandomForestRegressor\n",
    "model_RFR_NWP1 = RandomForestRegressor()\n",
    "\n",
    "# fit the model\n",
    "model_RFR_NWP1.fit(train_x, train_y)\n",
    "\n",
    "# plot feature importance\n",
    "plt.figure(figsize=(10,7))\n",
    "feat_importances = pd.Series(model_RFR_NWP1.feature_importances_, index = train_x.columns)\n",
    "feat_importances.nlargest(50).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature importance for NWP2\n",
    "\n",
    "# data\n",
    "X2 = X_train_cpy[['NWP2_wshear','NWP2_wdir_cos','NWP2_wdir_sin',\n",
    "                'hour_cos','hour_sin',\n",
    "                'month_sin','month_cos']]\n",
    "Y2 = y_train['Production']\n",
    "\n",
    "# split into train and test sets\n",
    "train_x2, test_x2, train_y2, test_y2 = train_test_split(X2, Y2, test_size=0.20, random_state=0)\n",
    "\n",
    "# instanciate object RandomForestRegressor\n",
    "model_RFR_NWP2 = RandomForestRegressor()\n",
    "\n",
    "# fit the model\n",
    "model_RFR_NWP2.fit(train_x2, train_y2)\n",
    "\n",
    "# plot feature importance\n",
    "plt.figure(figsize=(10,7))\n",
    "feat_importances = pd.Series(model_RFR_NWP2.feature_importances_, index = train_x2.columns)\n",
    "feat_importances.nlargest(50).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature importance for NWP3\n",
    "\n",
    "# data\n",
    "X3 = X_train_cpy[['NWP3_wshear','NWP3_wdir_cos','NWP3_wdir_sin',\n",
    "                'hour_cos','hour_sin','month_sin','month_cos']]\n",
    "Y3 = y_train['Production']\n",
    "\n",
    "# split into train and test sets\n",
    "train_x3, test_x3, train_y3, test_y3 = train_test_split(X3, Y3, test_size=0.20, random_state=0)\n",
    "\n",
    "# instanciate object RandomForestRegressor\n",
    "model_RFR_NWP3 = RandomForestRegressor()\n",
    "\n",
    "# fit the model\n",
    "model_RFR_NWP3.fit(train_x3, train_y3)\n",
    "\n",
    "# plot feature importance\n",
    "plt.figure(figsize=(10,7))\n",
    "feat_importances = pd.Series(model_RFR_NWP3.feature_importances_, index = train_x3.columns)\n",
    "feat_importances.nlargest(50).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cpy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature importance for NWP4\n",
    "\n",
    "# data\n",
    "X4 = X_train_cpy[['NWP4_CLCT','NWP4_wshear','NWP4_wdir_cos','NWP4_wdir','NWP4_wdir_sin', 'hour','month',\n",
    "                'hour_cos','hour_sin','month_sin','month_cos']]\n",
    "Y4 = y_train['Production']\n",
    "\n",
    "# split into train and test sets\n",
    "train_x4, test_x4, train_y4, test_y4 = train_test_split(X4, Y4, test_size=0.20, random_state=0)\n",
    "\n",
    "# instanciate object RandomForestRegressor\n",
    "model_RFR_NWP4 = RandomForestRegressor()\n",
    "\n",
    "# fit the model\n",
    "model_RFR_NWP4.fit(train_x4, train_y4)\n",
    "\n",
    "# plot feature importance\n",
    "plt.figure(figsize=(10,7))\n",
    "feat_importances = pd.Series(model_RFR_NWP4.feature_importances_, index = train_x4.columns)\n",
    "feat_importances.nlargest(50).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization  of the sub-feature space selected "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(\n",
    "    df_eda,\n",
    "    vars = ['NWP1_wshear', 'NWP1_T',\n",
    "           'NWP1_wdir_sin', 'NWP1_wdir_cos',\n",
    "           'hour_sin','hour_cos',\n",
    "           'month_sin','month_cos',\n",
    "           'Production'],\n",
    "    diag_kind='kde'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature transformations\n",
    "<a id=\"feat_transformation\"></a>\n",
    "\n",
    "#### Temperature convertion to $^{\\text{o}}$C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temps = ['NWP1_T', 'NWP3_T']\n",
    "\n",
    "dtr.convert_to_celsius(X_train_cpy, temps)\n",
    "dtr.convert_to_celsius(X_test_cpy, temps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Power  tranformations (Box-Cox and Yeo-Johnson)\n",
    "\n",
    "Some of the features, like wind speed, have skewed distributions. Box-Cox and Yeo-Johnson transformations can be used to get a more Gaussian like distribution. The Box-Cox family transformation is as follows:\n",
    "\n",
    "$$ \\begin{split}x_i^{(\\lambda)} =\n",
    "\\begin{cases}\n",
    "\\dfrac{x_i^\\lambda - 1}{\\lambda} & \\text{if } \\lambda \\neq 0, \\\\[8pt]\n",
    "\\ln{(x_i)} & \\text{if } \\lambda = 0,\n",
    "\\end{cases}\\end{split} $$\n",
    "\n",
    "where $\\lambda$ can be estimated from the traning data. This transformations can only be applied on positive data. \n",
    "\n",
    "Yeo-Johnson transformations can be applied on positive and negative data, given by:\n",
    "\n",
    "$$\n",
    "\\begin{split}x_i^{(\\lambda)} =\n",
    "\\begin{cases}\n",
    " [(x_i + 1)^\\lambda - 1] / \\lambda & \\text{if } \\lambda \\neq 0, x_i \\geq 0, \\\\[8pt]\n",
    "\\ln{(x_i + 1)} & \\text{if } \\lambda = 0, x_i \\geq 0 \\\\[8pt]\n",
    "-[(-x_i + 1)^{2 - \\lambda} - 1] / (2 - \\lambda) & \\text{if } \\lambda \\neq 2, x_i < 0, \\\\[8pt]\n",
    " - \\ln (- x_i + 1) & \\text{if } \\lambda = 2, x_i < 0\n",
    "\\end{cases}\\end{split}\n",
    "$$\n",
    "\n",
    "We'll try such transformations on our training data set, concretely Yeo-Johnson as we have some predictors with negative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# Yeo-Johnson transformation\n",
    "pt = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "x_trnsf = pt.fit_transform(df_eda[['NWP1_wshear', 'NWP1_T',\n",
    "           'NWP1_wdir_sin', 'NWP1_wdir_cos',\n",
    "           'hour_sin','hour_cos',\n",
    "           'month_sin','month_cos']])\n",
    "\n",
    "x_trnsf_df = pd.DataFrame(data=x_trnsf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(\n",
    "    x_trnsf_df,\n",
    "    diag_kind='kde'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.lambdas_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = PowerTransformer(method='box-cox', standardize=True)\n",
    "x_trnsf = pt.fit_transform(df_eda[['NWP1_wshear', 'NWP1_T']])\n",
    "x_trnsf_df = pd.DataFrame(data=x_trnsf)\n",
    "sns.pairplot(x_trnsf_df,diag_kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.lambdas_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "x_trnsf = ss.fit_transform(df_eda[['NWP1_wshear', 'NWP1_T']])\n",
    "x_trnsf_df = pd.DataFrame(data=x_trnsf)\n",
    "sns.pairplot(x_trnsf_df,diag_kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMax transformation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mm = MinMaxScaler()\n",
    "x_trnsf = mm.fit_transform(df_eda[['NWP1_wshear', 'NWP1_T',\n",
    "           'NWP1_wdir_sin', 'NWP1_wdir_cos',\n",
    "           'hour_sin','hour_cos',\n",
    "           'month_sin','month_cos']])\n",
    "\n",
    "x_trnsf_df = pd.DataFrame(data=x_trnsf)\n",
    "sns.pairplot(x_trnsf_df,diag_kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target transfomation\n",
    "<a id=\"target_transformation\"></a>\n",
    "\n",
    "The target (power production by the WF) has a very right skewed distribution also:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_eda['Production'], kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "y_trnsf = mm.fit_transform(np.array(df_eda['Production']).reshape(-1,1))\n",
    "\n",
    "sns.distplot(y_trnsf, kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also use a power transformation to handle the skeweness of the target distribution (more appropriate in this case than the log transformation, as the target has many 0 values)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers and abnormal data\n",
    "<a id=\"outliers\"></a>\n",
    "\n",
    "In weather parameters (except `CLCT`) there's no abnormal data, as we saw in notebook `1.1-vcp-explore-data`. We need to focus on the following four types of outliers:\n",
    "1. Data points in low wind speed period with high power generation.\n",
    "2. Data points with negtative value wind speed.\n",
    "3. Data points with negative value power generation.\n",
    "4. Data points with low power generation at high wind speed period.\n",
    "\n",
    "Let's have a quick look at the data sets to check types 2 and 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None\n",
    "df_eda[df_eda.columns[-38:]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There isn't any type 2 neither 3 outliers. However, looking at `CLCT`, which represents the cloud coverage percentage, we see the minimum value is negative, which makes nonsense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for val in df_eda['NWP4_CLCT'].sort_values(ascending=True):\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative values are very close to 0, it makes sense to replace all these observations by 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cpy.loc[X_train_cpy['NWP4_CLCT'] < 0, 'NWP4_CLCT'] = 0.0\n",
    "X_test_cpy.loc[X_test_cpy['NWP4_CLCT'] < 0, 'NWP4_CLCT'] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cpy['NWP4_CLCT'].sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying some ML algorithms to detect outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import models\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.abod import ABOD\n",
    "from pyod.models.cblof import CBLOF\n",
    "from pyod.models.hbos import HBOS\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.font_manager\n",
    "\n",
    "df = df_eda[['NWP1_wvel','Production']]\n",
    "\n",
    "mm = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "\n",
    "X1 = mm.fit_transform(np.array(df['NWP1_wvel']).reshape(-1,1))\n",
    "X2 = mm.fit_transform(np.array(df_eda['Production']).reshape(-1,1))\n",
    "\n",
    "#X1 = df['NWP1_wvel'].values.reshape(-1,1)\n",
    "#X2 = df['Production'].values.reshape(-1,1)\n",
    "X = np.concatenate((X1,X2), axis=1)\n",
    "\n",
    "random_state = np.random.RandomState(42)\n",
    "outliers_fraction = 0.10\n",
    "\n",
    "# Define seven outlier detection tools to be compared\n",
    "classifiers = {\n",
    "        'One-class SVM detector (OCSVM)': OCSVM(contamination=outliers_fraction), \n",
    "        #'Angle-based Outlier Detector (ABOD)': ABOD(contamination=outliers_fraction),\n",
    "        #'Isolation Forest': IForest(contamination=outliers_fraction,random_state=random_state, n_estimators=10000),\n",
    "        #'K Nearest Neighbors (KNN)': KNN(contamination=outliers_fraction, radius=0.05),\n",
    "        #'Average KNN': KNN(method='mean',contamination=outliers_fraction, n_neighbors=9)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xx , yy = np.meshgrid(np.linspace(0, 30 , 200), np.linspace(0, 12, 200))\n",
    "\n",
    "for i, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "    clf.fit(X)\n",
    "    # predict raw anomaly score\n",
    "    scores_pred = clf.decision_function(X) * -1\n",
    "        \n",
    "    # prediction of a datapoint category outlier or inlier\n",
    "    y_pred = clf.predict(X)\n",
    "    n_inliers = len(y_pred) - np.count_nonzero(y_pred)\n",
    "    n_outliers = np.count_nonzero(y_pred == 1)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    # copy of dataframe\n",
    "    dfx = df\n",
    "    dfx['outlier'] = y_pred.tolist()\n",
    "    \n",
    "    # IX1 - inlier feature 1,  IX2 - inlier feature 2\n",
    "    IX1 =  np.array(dfx['NWP1_wvel'][dfx['outlier'] == 0]).reshape(-1,1)\n",
    "    IX2 =  np.array(dfx['Production'][dfx['outlier'] == 0]).reshape(-1,1)\n",
    "    \n",
    "    # OX1 - outlier feature 1, OX2 - outlier feature 2\n",
    "    OX1 =  dfx['NWP1_wvel'][dfx['outlier'] == 1].values.reshape(-1,1)\n",
    "    OX2 =  dfx['Production'][dfx['outlier'] == 1].values.reshape(-1,1)\n",
    "         \n",
    "    print('OUTLIERS : ',n_outliers,'INLIERS : ',n_inliers, clf_name)\n",
    "        \n",
    "    # threshold value to consider a datapoint inlier or outlier\n",
    "    threshold = stats.scoreatpercentile(scores_pred,100 * outliers_fraction)\n",
    "        \n",
    "    # decision function calculates the raw anomaly score for every point\n",
    "    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()]) * -1\n",
    "    Z = Z.reshape(xx.shape)\n",
    "          \n",
    "    # fill blue map colormap from minimum anomaly score to threshold value\n",
    "    plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), threshold, 7),cmap=plt.cm.Blues_r)\n",
    "        \n",
    "    # draw red contour line where anomaly score is equal to thresold\n",
    "    a = plt.contour(xx, yy, Z, levels=[threshold],linewidths=2, colors='red')\n",
    "        \n",
    "    # fill orange contour lines where range of anomaly score is from threshold to maximum anomaly score\n",
    "    plt.contourf(xx, yy, Z, levels=[threshold, Z.max()],colors='orange')\n",
    "        \n",
    "    b = plt.scatter(IX1,IX2, c='white',s=20, edgecolor='k')\n",
    "    \n",
    "    c = plt.scatter(OX1,OX2, c='black',s=20, edgecolor='k')\n",
    "       \n",
    "    plt.axis('tight')  \n",
    "    \n",
    "    # loc=2 is used for the top left corner \n",
    "    plt.legend(\n",
    "        [a.collections[0], b,c],\n",
    "        ['learned decision function', 'inliers','outliers'],\n",
    "        prop=matplotlib.font_manager.FontProperties(size=20),\n",
    "        loc=2)\n",
    "      \n",
    "    plt.xlim((0, 30))\n",
    "    plt.ylim((0, 12))\n",
    "    plt.title(clf_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction of a datapoint category outlier or inlier\n",
    "y_pred = clf.predict(X)\n",
    "n_inliers = len(y_pred) - np.count_nonzero(y_pred)\n",
    "n_outliers = np.count_nonzero(y_pred == 1)\n",
    "\n",
    "# copy of dataframe\n",
    "dfx = df\n",
    "dfx['outlier'] = y_pred.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.array(df['NWP1_wvel']).reshape(-1,1)\n",
    "X2 = np.array(df_eda['Production']).reshape(-1,1)\n",
    "X = np.concatenate((X1,X2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "mm = StandardScaler()\n",
    "\n",
    "X1 = mm.fit_transform(np.array(df['NWP1_wvel']).reshape(-1,1))\n",
    "X2 = mm.fit_transform(np.array(df_eda['Production']).reshape(-1,1))\n",
    "X = mm.fit_transform(np.concatenate((X1,X2), axis=1))\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=15, min_samples=25).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = pd.Series(clusterer.outlier_scores_).quantile(0.96)\n",
    "outliers = np.where(clusterer.outlier_scores_ > threshold)[0]\n",
    "plt.scatter(*X.T, s=50, linewidth=0, c='gray', alpha=0.20)\n",
    "plt.scatter(*X[outliers].T, s=50, linewidth=0, c='red', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_palette = sns.color_palette()\n",
    "cluster_colors = [color_palette[x] if x >= 0 else (0.5, 0.5, 0.5) for x in clusterer.labels_]\n",
    "cluster_member_colors = [sns.desaturate(x, p) for x, p in\n",
    "                         zip(cluster_colors, clusterer.probabilities_)]\n",
    "plt.scatter(*X.T, s=50, linewidth=0, c=cluster_member_colors, alpha=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer.labels_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## With DBSCAN \n",
    "from sklearn.cluster import DBSCAN\n",
    "outlier_detection = DBSCAN(\n",
    "    eps = 0.1,\n",
    "    metric=\"l1\",\n",
    "    min_samples = 10,\n",
    "    n_jobs = -1)\n",
    "\n",
    "\n",
    "mm = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "\n",
    "X1 = mm.fit_transform(np.array(df['NWP1_wvel']).reshape(-1,1))\n",
    "X2 = mm.fit_transform(np.array(df_eda['Production']).reshape(-1,1))\n",
    "X = np.concatenate((X1,X2), axis=1)\n",
    "\n",
    "clusters = outlier_detection.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = np.where(clusters == -1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(*X.T, s=50, linewidth=0, c='gray', alpha=0.20)\n",
    "plt.scatter(*X[outliers].T, s=50, linewidth=0, c='red', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store X_train_cpy\n",
    "%store X_test_cpy\n",
    "%store y_train\n",
    "%store y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WindPowerForecasting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
